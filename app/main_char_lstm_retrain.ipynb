{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588313 [['jules', 'verne', 'mil', 'podmorskiej', 'żeglugitłum'], ['tłumacz', 'nieznanyisbn', 'skała', 'uciekającanie', 'zapomniano', 'zapewne', 'dotąd', 'wypadku', 'dziwnego', 'niepojętego', 'i', 'trudnego', 'do', 'objaśnienia', 'zjawiska', 'jakim', 'się', 'odznaczył', 'rok'], ['nie', 'mówiąc', 'już', 'o', 'pogłoskach', 'niepokojących', 'ludność', 'portów', 'i', 'zajmujących', 'ogół', 'na', 'wszystkich', 'lądach', 'dodać', 'wypada', 'że', 'marynarze', 'byli', 'najmocniej', 'zaniepokojeni'], ['kupcy', 'armatorzy', 'dowódcy', 'okrętów', 'szyprowie', 'i', 'sternicy', 'statków', 'europejskich', 'i', 'amerykańskich', 'oficerowie', 'marynarki', 'wojennej', 'wszystkich', 'krajów', 'a', 'nawet', 'rządy', 'różnych', 'państw', 'obu', 'lądów', 'do', 'najwyższego', 'stopnia', 'zajęci', 'byli', 'tym', 'wydarzeniem'], ['od', 'niejakiego', 'czasu', 'okręty', 'napotykały', 'na', 'morzu', 'jakąś', 'rzecz', 'ogromną', 'przedmiot', 'długi', 'kształtu', 'wrzecionowatego', 'niekiedy', 'świecący', 'nieskończenie', 'większy', 'i', 'szybszy', 'od', 'wieloryba']]\n",
      "630691 ['jules verne mil podmorskiej żeglugitłum', 'tłumacz nieznanyisbn skała uciekającanie zapomniano zapewne dotąd wypadku dziwnego niepojętego i trudnego do objaśnienia zjawiska jakim się odznaczył rok', 'nie mówiąc już o pogłoskach niepokojących ludność portów i zajmujących ogół na wszystkich lądach dodać wypada że marynarze byli najmocniej zaniepokojeni', 'kupcy armatorzy dowódcy okrętów szyprowie i sternicy statków europejskich i amerykańskich oficerowie marynarki wojennej wszystkich krajów a nawet rządy różnych państw obu lądów do najwyższego', 'stopnia zajęci byli tym wydarzeniem']\n"
     ]
    }
   ],
   "source": [
    "# keras module for building LSTM\n",
    "from keras.utils import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "\n",
    "# set seeds for reproducability\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os\n",
    "import json as pjson\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import tokenizer_from_json\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "from dataset import load_tokenized_sentences\n",
    "\n",
    "sentence_len_min_limit = 2\n",
    "sentence_len_max_limit = 25\n",
    "\n",
    "dataset1 = load_tokenized_sentences(\"../datasets/pickled/books_clear.pickle\") \n",
    "dataset2 = load_tokenized_sentences(\"../datasets/pickled/bajki-extebd_clear.pickle\") \n",
    "dataset = dataset1 + dataset2\n",
    "print(len(dataset), dataset[:5])\n",
    "\n",
    "dataset_join = []\n",
    "for sentence in dataset:\n",
    "    if len(sentence) > sentence_len_max_limit:\n",
    "        for i in range(0, len(sentence) - sentence_len_min_limit, sentence_len_max_limit):\n",
    "           dataset_join.append(\" \".join(sentence[i:i+sentence_len_max_limit])) \n",
    "    if sentence_len_min_limit <= len(sentence) <= sentence_len_max_limit:\n",
    "        dataset_join.append(\" \".join(sentence))\n",
    "dataset = dataset_join\n",
    "\n",
    "print(len(dataset), dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[62859, 103749, 1245], [62859, 103749, 1245, 33519], [62859, 103749, 1245, 33519, 171004], [21370, 88416, 9660], [21370, 88416, 9660, 171005], [21370, 88416, 9660, 171005, 13608], [21370, 88416, 9660, 171005, 13608, 532], [21370, 88416, 9660, 171005, 13608, 532, 539], [21370, 88416, 9660, 171005, 13608, 532, 539, 1731], [21370, 88416, 9660, 171005, 13608, 532, 539, 1731, 2222]]\n",
      "(6483631, 24) (6483631,)\n",
      "(256, 24) (256,)\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def get_sequence_of_tokens(dataset: List[str], tokenizer: Tokenizer = None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(dataset)\n",
    "\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    word2int_sequences = []\n",
    "    for line in dataset:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(sentence_len_min_limit, len(token_list)):\n",
    "            n_gram_sequence = token_list[: i + 1]\n",
    "            word2int_sequences.append(n_gram_sequence)\n",
    "    return word2int_sequences, total_words, tokenizer\n",
    "\n",
    "\n",
    "def generate_padded_sequences(word2int_sequences: List[List[int]]):\n",
    "    max_sequence_len = max([len(x) for x in word2int_sequences])\n",
    "    word2int_sequences = np.array(\n",
    "        pad_sequences(word2int_sequences, maxlen=max_sequence_len, padding=\"pre\")\n",
    "    )\n",
    "    predictors, label = word2int_sequences[:, :-1], word2int_sequences[:, -1]\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "\n",
    "def dataset_generator(predictors, label, batch_size = 256, shuffle = True):\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            p = np.random.permutation(len(predictors))\n",
    "        else:\n",
    "            p = np.arange(len(predictors))\n",
    "        for i in range(0, len(predictors) - batch_size + 1, batch_size):\n",
    "            indexes = p[i : i + batch_size]\n",
    "            yield predictors[indexes], label[indexes]\n",
    "\n",
    "with open('../lstm_models/tokenizer.json', 'r') as f:\n",
    "    data = pjson.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "    \n",
    "word2int_sequences, total_words, tokenizer = get_sequence_of_tokens(dataset, tokenizer)\n",
    "print(word2int_sequences[:10])\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(word2int_sequences)\n",
    "print(predictors.shape, label.shape)\n",
    "\n",
    "batch_size = 256\n",
    "gen = dataset_generator(predictors, label, 256, True)\n",
    "a, b = next(gen)\n",
    "print(a.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331307 25\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 24, 50)            16565350  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 24, 128)           91648     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 24, 256)           394240    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 512)               1574912   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 331307)            169960491 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 188,586,641\n",
      "Trainable params: 171,535,403\n",
      "Non-trainable params: 17,051,238\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from keras import optimizers\n",
    "\n",
    "class PredictCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, seed_text, next_words, max_sequence_len, randomize=True, max_randomize_words = 2):\n",
    "        self.seed_text = seed_text\n",
    "        self.next_words = next_words\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "        self.randomize = randomize\n",
    "        self.max_randomize_words = max_randomize_words\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        seed_text: str = self.seed_text\n",
    "        for _ in range(self.next_words):\n",
    "            token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "            token_list = pad_sequences(\n",
    "                [token_list], maxlen=self.max_sequence_len - 1, padding=\"pre\"\n",
    "            )\n",
    "            predicted = self.model.predict_on_batch(token_list)[0]\n",
    "            if self.randomize:\n",
    "                indc = np.argpartition(predicted, -self.max_randomize_words)[-self.max_randomize_words:]\n",
    "                predicted = random.choice(indc)\n",
    "            else:\n",
    "                predicted = predicted.argmax()\n",
    "            \n",
    "            output_word = \"\"\n",
    "            for word, index in tokenizer.word_index.items():\n",
    "                if index == predicted:\n",
    "                    output_word = word\n",
    "                    break\n",
    "            seed_text += \" \" + output_word\n",
    "        print(f\"Start epoch {epoch} of training; Generated text:\", seed_text)\n",
    "\n",
    "\n",
    "def create_model(max_sequence_len: int, total_words: int) -> Sequential:\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 50, input_length=input_len))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(512))\n",
    "    model.add(Dense(total_words, activation=\"softmax\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizers.Adam(1e-4))\n",
    "    return model\n",
    "\n",
    "print(total_words, max_sequence_len)\n",
    "    \n",
    "model = create_model(max_sequence_len, total_words)\n",
    "model.load_weights('../lstm_models/model_best_2.h5')\n",
    "for layer in model.layers[:-2]:\n",
    "    layer.trainable = False\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 0 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu ale już już dawno było coś bolesnego i nieustającym w tym celu że nie mógł już sobie dać rady z resztą czasu i sam z\n",
      "Start epoch 0 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu i już nie było nikogo kto by się nie zapamiętał i nie mógł pojąć począł krzyczeć wniebogłosy bo nie mógł się powstrzymać od uniesienia i\n",
      "Epoch 1/10\n",
      "25326/25326 [==============================] - 1760s 69ms/step - loss: 3.9509\n",
      "Start epoch 1 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu a potem położyła ją na stole odwinęła z chustki garronego i drżącymi rękami nie chciało się mówić padłem na nich i pocałowała go za ramię\n",
      "Start epoch 1 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu i uśmiechała się do mnie z gorącą uciechą i z przejęciem w spojrzeniu przyczem gdy się już nie dotarło do tego policzka połyskiwały sine rumieńce\n",
      "Epoch 2/10\n",
      "25326/25326 [==============================] - 1758s 69ms/step - loss: 3.8972\n",
      "Start epoch 2 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu i nie wiadomo czy kum i co dzień jeździła na północ do mohilowa i posuwali się dopiero z dwu stron z wielkim trudem w szwedzkim\n",
      "Start epoch 2 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu i uśmiechała się do mnie z cicha i z taką zapamiętałością że hance łzy kapały z utrudzenia usiadł i chwyciwszy się z barłogu runął na\n",
      "Epoch 3/10\n",
      "25326/25326 [==============================] - 1759s 69ms/step - loss: 3.8786\n",
      "Start epoch 3 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu a on już był w chałupie a na końcu leżał na wznak w podle chałupy bieganina i grube kożuchy rwały z ziemi owies z wełny\n",
      "Start epoch 3 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu i uśmiechała się do mnie z cicha i z taką zapamiętałością że hance łzy kapały z utrudzenia usiadł i chwyciwszy z ciekawości popatrzyła na nią\n",
      "Epoch 4/10\n",
      "25326/25326 [==============================] - 1765s 70ms/step - loss: 3.8667\n",
      "Start epoch 4 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu i nie mogła się ruszyć z upokarzającego skrzętu i tak się odwracać że szedł do izb z której się rzuciło że to w porządku i\n",
      "Start epoch 4 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu i uśmiechała się do mnie i wołała oj chryste mój błogosław wojenniku tuli wódki po polsku i po zachodzie słońca chował się w kącie i\n",
      "Epoch 5/10\n",
      "25326/25326 [==============================] - 1759s 69ms/step - loss: 3.8571\n",
      "Start epoch 5 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu i nie mogła się powstrzymać od patrzenia w ogień i nuż się na ziemię utrzymać i kajał go z tym grubasem który był ten stos\n",
      "Start epoch 5 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu i uśmiechała się do mnie z cicha i z żalu i strachu jęła się zbierać z łóżka i pobiegła do kościoła i zastawszy drzwi zamknięte\n",
      "Epoch 6/10\n",
      "25326/25326 [==============================] - 1784s 70ms/step - loss: 3.8482\n",
      "Start epoch 6 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu i uśmiechała głowę do sąsiedniej komory skoczyła ku niej i ostrożnie szepcząc głośno o matkę i po raz pierwszy w życiu i z całej duszy\n",
      "Start epoch 6 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu i uśmiechała się do mnie i pyta czy nie idziesz z nami do raszkowa nie wywiozę cię do kryminału do więzienia i będziesz miał w\n",
      "Epoch 7/10\n",
      "25326/25326 [==============================] - 1826s 72ms/step - loss: 3.8381\n",
      "Start epoch 7 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu i uśmiechała głowę do oleńki uciekła z izby na jarmark i tam przepada za wiatrem gdzie pożar gdzie pokotem gdzie miejscowi gdzie strzelcy w sukmanach\n",
      "Start epoch 7 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu i uśmiechała się do niej i napawała ją coraz więcej i nie mógł się powstrzymać od patrzenia na niego i zbiegła w głąb wąwozu i\n",
      "Epoch 8/10\n",
      "25326/25326 [==============================] - 1859s 73ms/step - loss: 3.8315\n",
      "Start epoch 8 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu i tam już nie było w izbie niani i naturalnie jej się nie spodobały ani słowa jej nie mogła było w niej pomieścić ani w\n",
      "Start epoch 8 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu i uśmiechała się do niego i lękliwie i z cicha że w tej samej chwili złapał się za głowę i zajrzała do niej jakby w\n",
      "Epoch 9/10\n",
      "25326/25326 [==============================] - 1796s 71ms/step - loss: 3.8242\n",
      "Start epoch 9 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu a on był tak potrzebnym i przystojny i pełen zasadzek i że w sercu swoim nie doznała od niej zwyczaju to jednak że nie ma\n",
      "Start epoch 9 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu i uśmiechała się do niego i lękliwie i chrapiąc jak to było już na świecie że nie wiada co było z nią począć i zalewał\n",
      "Epoch 10/10\n",
      "11039/25326 [============>.................] - ETA: 17:49 - loss: 3.7959"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    gen,\n",
    "    steps_per_epoch=len(predictors) // batch_size,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        PredictCallback(\"dawno temu czerwony kapturek poszedł do lasu\", 25, max_sequence_len, True),\n",
    "        PredictCallback(\"dawno temu czerwony kapturek poszedł do lasu\", 25, max_sequence_len, False),\n",
    "        tf.keras.callbacks.ModelCheckpoint('../lstm_models/model_best_2_retrained.h5', monitor='loss', save_best_only=True, save_weights_only=True)\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 0 of training; Generated text: dawno temu czerwony kapturek poszedł do lasu i już nie mogła się oprzeć wrażeniu i uniosła go do ziemi a ona nie mogła się jej z tym pogodzić na ziemię i na oślep do domu a potem w południe i w tej samej izbie z drugiej z nich pasło się w gęstwinę głazach w zielonych gniazdach się z nim rosła a na końcu królewskim rosła i przygotowywano w górę a ziarno w lesie stuknęło jak obłoki jęło w spienioną ulewę i w końcu się dzieje i wrzawa dzwonów i organy żałośnie chciały i w ręku się cisną a w nich się z nią wszystko co ściągnęło ramionami a potem zmykaj mi na pomoc i wracaj do mnie i powiedz mi wszystko co się stało w tej komnacie i że nie mogę ci dać rady ani na moment bo to nie ma nic przeciwko temu że to jest wróżba i że w tej chwili jest ci się niegodni i że nie ma ani grosza ani na gorycz pierścienia i nie może być z tego powodu że to jest dla mnie obojga co do tego co się nazywa i z tego nie mam nic wspólnego ze sobą nie mogę być moim mężem a ty nie jesteś tak ciekawa głupi że jesteś człowiekiem subtelnym czy córeczko nie mogłabym się nawet dziwić czy nie ma serca i przyzwyczajenia ale nie wiem gdzie jest ukryta w mym sercu czy nie ma nic przeciwko pośpiechowi odparłem pożerając potajemnie w obydwóch diamentów w której się przeziębił na kucharza i sierot smakowało jej drogie dziecię a potem przynieś ci je na przechowanie do jedzenia i dukatów i sery nie było już gotowe na siew a rankiem na obiad do macieja widu i pastuch aresztowali nie mógł się doczekać głosu kołowrotek zapłonęły w nozdrza kota a w końcu ludożerca spadły z jej drzwi i w końcu się płaczem na ziemię skrępowali z ziemi i na wierzchołkach drzewa i jasiowy się do domu a gdy się nić waliły i huczało w ciemnościach jakby się kto był tkwiący w powietrzu nie było już chałupie ani na warząchwi ani na chwilę nie wiedziało jeno o co ni się nie mieszał bo nie miał czasu do stracenia i do wieczora i z widocznym lękiem że nie mógł go nawet dźwignąć do niej w chwili gdy się jej dowiedziały że nie może być w stanie na pewno nie omieszkał ustąpić i nie zastanawiając się ani chwili nie odważyła się nawet stłumić w dłoni i powstrzymać zwierzę ale nadaremno zabijał go w domu i że w pobliżu tego gnającego będzie na ziemi na ziemi w którym panuje nasz los i żałoba moje będzie się psuł i nie zdążę i nie będę mogła sobie wyobrazić co to było z jego winy a w końcu nie było już nic prócz ślepego błękitu w którym mieszkała maleńka i pogodna noc i ciemność zupełna jak makiem siał pochylała sią i magdusia zapragnęła i gołąbka na niej się kończy i w mig się dzieje a w tym jabłku szalu i w tym beczce górawiodły go ścigały strzępy\n",
      "Start epoch 0 of training; Generated text: sierotka weszła do domu i zobaczyła siedzącą przy stolę babcię która była w niej i że ją nie było i spytała czy nie zechciałaby się w tym regimencie zostać w praniu wieszczki wybrać mi to czemu to ja jestem tak szczera że dziewczynka nie chciała się na śmierć nie dzielić w ręce bo na niego oderwać ablucji i wszelakiej włóczni nie przypadły do siebie i tak jak poprzednia jak majstrowa nić w której prześwieca się woda w źródle którą się tak wyuczył jak to się w tym ślicznym czasie i że zięć był w istocie nikczemnym tyranem i żądny łaskawości i jego sąsiadów z królestwa guru okrągłego owczarka jabłko bierze od tego czarnoksiężnika który tam był w pobliżu mego ojca i tak się nadworną z nim juro że cię zabiję a tu będzie na miejscu i na górze a ja będę miał zdarty się stracha i nie mógł się połapać z nim i odezwać aż do rana po zakupy niańka że nie ma już nic do zjedzenia i nie zastanawiając go bramin i opowiedział o swoim urodzeniu i że nie ma nic prócz życzenia żeby się spełniło i hans rękami nie pozostawić w sobie w domu i w ogóle w tym samym czasie i nie ma ani jednego kwiatka na gałęzi i kóz w gościnie w lesie o wczesnej godzinie na północ od morza i w nocy i w nocy w południe od czasu jak się płynęło radosna i zakurzona snująca się na dach i słuchała opowieści i o tym co się stało z tym samym polem gdzie się chłopskim trafem nie widział i nie ma co się z niego stało ba on się z nim ożeni z tym samym panem a teraz nie mogę tego zrobić bo nie straciłam prawa i na to nie ma się na to wdawać obdarzając mnie tak wielka i tak jakbym mógł mi pomóc w tej chwili do oczu się nie śmiały bo nie było mi nic złego a nie mam ani kawałeczka pigułki bo nie mogę się oprzeć pokusie i rzekła macocha nie chcę być moim mężem i nie chcę ci dać rady za moją niegrzeczność a wtedy do tego z tego że w końcu straciłem cierpliwość i obiecała mi się najeść na próbę i że nie może być w domu i nie chcę się nadziękować jego imię bo diabłem młodszego najstarszego syna jest to córka króla który nie ma mocy do życia na ziemi a potem nie chcę się doprosić ani za jednym z nich nie było nic innego do tego co by mógł się nasycić i nadaremnie i szarpać się zawistna nie podzielić mnie na miejscu i że nie ma już nic do stracenia a ja nie mam ani jednego wilka i nie mam ani jednego przyzwoitego człowieka ani kawałeczka drobiu i w tej mysiej norze z błota i nakarmiła mi zjem cudowne i zjedzone się w popiół na dachu a w niej usadowiła się w nim i z siekierą w garści i łąkach w niej czwórki powrozy malcy magdusia z kory się pasły a w ślad za nimi rosła ludzie\n",
      "Start epoch 0 of training; Generated text: była sobie zła królowa która chciała panować nad światem a ona przytuliła ją na szyję sułtanowi że zapomniała o tym jak ją w łóżeczku na półmisek królewski krzyknąwszy a za nią zaciekawiona że to pietrzyk z niego się dzieje i nie mógł się powstrzymać od śmiechu i że nie ma w domu ani jednego wilka ani jednego z tych królewien które baśni w dzieciństwie uległ swemu wszystkim co to jest w dzieciństwie a literatura traktowana była w ciągłej pracy i piękniejsza od hulanki świata i wiecznych śniegów i nie może być w domu i nie mógł się doprosić za klasyczną i łupem i że w tej kolejnej koronie z jego oczu i w turkocie chaty uderzały toma do dziecinnego brzegu a gdy janek zwątpił na chwilę niańka w skoku do domu w lesie i w izdebce na poddaszu na krętych miedzy szumiała z powrotem do zagrody w którym mieszkał król z królową w którym mieszkał roeoender w domu pasał z jarmarku w karty i z rynsztunku i wrócił z żoną do domu w którym mieszkał nadworny krawiec i piękna żona oczekiwała salę napełnione diabłami i myszy zaczęły spływać po wąskich ciemnych wsi i izbach z winem i owoców służące do domu tańczyły w głębinie wody i kwiaty z pierza na ziemię i zioła posnęli i ochoczo nie było nikogo kto okazałby mu drogę i nie było nic innego w legendach i nie znalazł ani chwili ani połowy zwłok literackiej nie mieściła mi się ani śladu życia nie zastanawiali na tym nic złego zachowałem tylko w tej samej nocy bramin z trzema towarzyszami a potem na pięcie i obrus na kominku nieżywy był pochmurny i dźwięczny a w dodatku warzył jak tylko pomaszerował na podłogę z których zasłaniał się tak elegancko jak w paszczy olbrzyma i tak się kochali że nie było w domu ani jednego przyzwoitego chleba i że nie mógł znaleźć ani śladu podobieństwa które by się nie obudziły i nikt się nie bał to nie było tak wyszukane jak masło urodzonym w grocie na dnie oceanu i oceany jak srebro a w dodatku w skarbcu odwrotnym szeregiem małych tekstów i drobnych głazach z wszelkich stron górskiego sic grenadierów w cudowną sen w różane baśniach w przeciwieństwie do środkowych głównej siedział w pobliżu srebrnych kurcząt i srebrnych jabłek jak w złocistych oczach i zielone i siwych zębów wiedźmy owinięte w których błyszczały płatki i żółta złota wydzielające z gałązek ziarnka na trawę na gałęziach i trawie na nogach a następnie wyczekałem się ze mnie rycerz który ma w kieszeni śliczną babę i z woreczka ulepił ząb kapitanowi i ugryźć w poranek popiele ukryty na niebie w koronie z trojgiem kija i w mgnieniu boju z zanadrza w ręku zakopanego kielichy i dzbany z sierścią pościelą gęsią snopek i ja także matka pięknej szkatuły to ci potwornych jak w maśle a w nich spanie w trzewikach w której więzły w uszach jak gdyby się bieli z niedźwiedziem i nie mógł oderwać się od niego i uciekł do domu w potoku wdrapał mu go w oczy i oddawała mu pokłon\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('../lstm_models/model_best_2_retrained.h5')\n",
    "\n",
    "texts = [\n",
    "    \"dawno temu czerwony kapturek poszedł do lasu\",\n",
    "    \"sierotka weszła do domu i zobaczyła siedzącą przy stolę babcię\",\n",
    "    \"była sobie zła królowa która chciała panować nad światem\"\n",
    "]\n",
    "for text in texts: \n",
    "    callb = PredictCallback(text, 500, max_sequence_len, True)\n",
    "    callb.model = model\n",
    "    callb.on_epoch_begin(0)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
