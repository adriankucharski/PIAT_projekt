{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import *\n",
    "output_dataset_folder = \"../datasets/pickled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import *\n",
    "dataset = read_texts('../texts/bajki-extend')\n",
    "save_tokenized_sentences(dataset, '../datasets/words/bajki-raw.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-09 19:32:54.839035: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-09 19:32:55.348608: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import json\n",
    "from dataset import *\n",
    "import re\n",
    "\n",
    "num_words = 50_000\n",
    "extra_regex_chars='.,?!:;()-'\n",
    "token_regex = '„”!\"#$%&()*+,-—./:;<=>?@[\\\\]^_`{|}~0123456789…°'\n",
    "\n",
    "for c in extra_regex_chars:\n",
    "    token_regex = token_regex.replace(c, '')\n",
    "\n",
    "def clear_dataset(dt: List[str], to_left: str = r' A-Za-ząćęłńóśźż\\-.,?!:;()\\n', to_lower = True) -> List[str]:\n",
    "    new_dt = [re.sub(f'[^{to_left}]+', '', text.lower() if to_lower else text) for text in dt]\n",
    "    return new_dt\n",
    "\n",
    "val = read_texts('../texts/val')\n",
    "dataset2 = read_texts('../texts/bajki-extend')\n",
    "dataset1 = read_texts('../texts/books-raw')\n",
    "dataset = dataset1 + dataset2\n",
    "\n",
    "\n",
    "# for i in range(len(dataset)):\n",
    "#     dataset[i] = re.sub(r'(['+extra_regex_chars+'])', r' \\1', dataset[i])\n",
    "\n",
    "\n",
    "# dataset = clear_dataset(dataset)\n",
    "# val = clear_dataset(val)\n",
    "# save_tokenized_sentences(dataset, '../datasets/words/books-bajki-raw.pickle')\n",
    "# save_tokenized_sentences(val, '../datasets/words/books-bajki-raw-val.pickle')\n",
    "\n",
    "tokenizer = Tokenizer(num_words, token_regex)\n",
    "tokenizer.fit_on_texts(dataset)\n",
    "with open(f'../datasets/words/books-bajki-raw-tokenizer_{num_words}.json', 'w', encoding='utf8') as f:\n",
    "   f.write(json.dumps(tokenizer.to_json(), ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50), dtype=int64, numpy=\n",
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0]], dtype=int64)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import TextVectorization\n",
    "from dataset import *\n",
    "val = read_texts('../texts/val')\n",
    "\n",
    "num_words = 100_000\n",
    "args = {\n",
    "    \"max_tokens\": num_words,\n",
    "    \"standardize\": \"lower_and_strip_punctuation\",\n",
    "    \"output_mode\": \"int\",\n",
    "    \"output_sequence_length\": 50\n",
    "}\n",
    "vec = TextVectorization(**args)\n",
    "\n",
    "vec.adapt(val)\n",
    "\n",
    "vec.call([\"adrian\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#Let's add custom sentences \n",
    "sentences = [\n",
    "    \"Apples are red\",\n",
    "    \"Apples are round\",\n",
    "    \"Oranges are round\",\n",
    "    \"Grapes are green\"\n",
    "]\n",
    "\n",
    "#Tokenize the sentences using OOV\n",
    "myTokenizer = Tokenizer(num_words=100)\n",
    "myTokenizer.fit_on_texts(sentences)\n",
    "print(myTokenizer.word_index)\n",
    "\n",
    "# Unseen Words\n",
    "test_data = [\n",
    "    'Grapes are sour but oranges are sweet',\n",
    "]\n",
    "\n",
    "test_seq = myTokenizer.texts_to_sequences(test_data)\n",
    "print(\"\\nTest Sequence = \", test_seq, \" => \", [x for x in myTokenizer.sequences_to_texts_generator(test_seq)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from keras.models import Model, load_model\n",
    "from lstm import *\n",
    "from keras.preprocessing.text import tokenizer_from_json\n",
    "import json \n",
    "\n",
    "with open('../datasets/words/books-bajki-raw-tokenizer_100000.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "      \n",
    "\n",
    "model: Model = load_model('../lstm_models/model_best_5.h5')        \n",
    "text_seed = \"dawno temu czerwony kapturek poszedł do lasu\"\n",
    "\n",
    "for temperature in tqdm([0, 0.1, 0.2, 0.3, 0.5, 0.8, 1.0]):\n",
    "    pr = PredictCallback(text_seed, 50, LEN_MAX_LIMIT, tokenizer, temperature, model)\n",
    "    text = pr.generate_text()\n",
    "    with open(f'../generated_texts/lstm/text_temperature_{temperature:.2f}.txt', 'w', encoding='utf8') as f:\n",
    "        f.write(f'Seed: {text_seed}\\n')\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14 11 10  8  5]\n",
      " [ 6  2 10  5  0]\n",
      " [ 4  4  1 10  3]\n",
      " [ 0  2  6  5  4]\n",
      " [ 6  0  1  6 14]\n",
      " [ 8  3  0 11  2]\n",
      " [ 4  1  0  0  0]\n",
      " [ 3  0  4  0 13]\n",
      " [ 5 10  7 10 12]\n",
      " [ 3 14 14  8  2]\n",
      " [10  5  2  2  4]\n",
      " [12  9  6  1 13]\n",
      " [13  2  8  9  8]\n",
      " [ 7 13  3  9  7]\n",
      " [12 11  3  1 10]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[14,  0, 10,  8,  5],\n",
       "       [ 0,  2, 10,  5,  0],\n",
       "       [ 4,  4,  0, 10,  3],\n",
       "       [ 0,  2,  6,  0,  4],\n",
       "       [ 6,  0,  1,  6, 14],\n",
       "       [ 8,  3,  0, 11,  2],\n",
       "       [ 4,  1,  0,  0,  0],\n",
       "       [ 3,  0,  4,  0, 13],\n",
       "       [ 5, 10,  7, 10, 12],\n",
       "       [ 3, 14, 14,  8,  2],\n",
       "       [10,  5,  2,  2,  4],\n",
       "       [12,  9,  6,  1, 13],\n",
       "       [13,  2,  8,  9,  8],\n",
       "       [ 7, 13,  3,  9,  7],\n",
       "       [12, 11,  3,  1, 10]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.randint(0, 15, size=(15, 5))\n",
    "print(a)\n",
    "ind = np.asarray([1, 0, 2, 3, 1]) \n",
    "ind = np.stack([np.arange(len(ind)), ind]).T\n",
    "a[ind[:, 0], ind[:, 1]] = 0\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
