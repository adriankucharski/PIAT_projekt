{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-10 15:29:19.924449: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-10 15:29:20.395968: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.get_visible_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'size': 145, 'tokenizer': 100000, 'str': 'louisa may alcott\\n\\nmałe kobietki\\ntłum. zofia grabowska\\n\\nisbn ----\\n\\n\\n\\n\\nrozdział i. pielgrzymki\\n\\n co mi za boże narodzenie bez podarków!  mruknęła ludka, leżąc na dywanie przed kominkiem.\\n\\n to straszne być ubogą!  westchnęła małgosia, spoglądając na swą starą suknię.\\n\\n bardzo jest nieładnie, że niektóre dziewczęta mają mnóstwo pięknych rzeczy, a inne nie mają nic  dodała amelka z gniewną minką.\\n\\n mamy przecież ojca, mamę i siebie nawzajem  z zadowoleniem odezwała się eliza ze swego kącika.\\n\\nich cz'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "from keras.preprocessing.text import tokenizer_from_json\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "from dataset import *\n",
    "\n",
    "dataset = load_tokenized_sentences('../datasets/words/books-bajki-raw.pickle')\n",
    "\n",
    "with open('../datasets/words/books-bajki-raw-tokenizer_100000.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "        \n",
    "print({\n",
    "    \"size\": len(dataset),\n",
    "    \"tokenizer\": tokenizer.num_words,\n",
    "    \"str\": dataset[1][:500],\n",
    "    \"tokenizer\": tokenizer.num_words\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import (\n",
    "    Layer,\n",
    "    MultiHeadAttention,\n",
    "    Dense,\n",
    "    LayerNormalization,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    Input,\n",
    "    Add\n",
    ")\n",
    "from keras import Model, losses, Sequential, callbacks, activations, optimizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "from typing import Literal\n",
    "import numpy as np\n",
    "\n",
    "class MaskedSparseCategoricalCrossentropy(losses.Loss):\n",
    "    def __init__(self, from_logits: bool = True, pad_value: int = 0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pad_value = pad_value\n",
    "        self.loss = losses.SparseCategoricalCrossentropy(from_logits, reduction=\"none\")\n",
    "\n",
    "    def call(self, y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        mask = tf.cast(y_true != self.pad_value, dtype=loss.dtype)\n",
    "        loss *= mask\n",
    "        loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(\n",
    "        self, embed_dim: int, num_heads: int, ff_dim: int, rate: float = 0.1, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "\n",
    "        self.dropout1 = Dropout(self.rate)\n",
    "        self.dropout2 = Dropout(self.rate)\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6, center=True, scale=True)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6, center=True, scale=True)\n",
    "\n",
    "        self.mha = MultiHeadAttention(self.num_heads, self.embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [\n",
    "                Dense(self.ff_dim, activation=\"relu\"),\n",
    "                Dense(self.embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def attention_mask(self, batch_size: int, n_dest: int, n_src: int, dtype: tf.DType) -> tf.Tensor:\n",
    "        i = tf.expand_dims(tf.range(n_dest), axis=-1)\n",
    "        j = tf.range(n_src)\n",
    "        mask = tf.cast(i >= j - n_src + n_dest, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
    "        batch_size, seq_len = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "        mask = self.attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        attention_output = self.mha(inputs, inputs, attention_mask=mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out = self.layernorm1(inputs + attention_output)\n",
    "        ffn_out = self.ffn(out)\n",
    "        ffn_out = self.dropout2(ffn_out)\n",
    "        norm = self.layernorm2(out + ffn_out)\n",
    "        return norm\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        config = {\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"rate\": self.rate,\n",
    "        }\n",
    "\n",
    "        return config\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, max_len: int, vocab_size: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.positions = tf.range(start=0, limit=self.max_len, delta=1)\n",
    "        self.embedding_token = Embedding(input_dim=self.vocab_size, output_dim=self.embed_dim, mask_zero=True)\n",
    "        self.embedding_position = Embedding(input_dim=self.max_len, output_dim=self.embed_dim)\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        positions = self.embedding_position(self.positions)\n",
    "        x = self.embedding_token(x)\n",
    "        _sum = x + positions\n",
    "        return _sum\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        config = {\n",
    "            \"max_len\": self.max_len,\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "        }\n",
    "        return config\n",
    "\n",
    "\n",
    "class TextGenerator(callbacks.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seed_text: str,\n",
    "        next_words: int,\n",
    "        max_sequence_len: int,\n",
    "        tokenizer: Tokenizer,\n",
    "        top_k=10,\n",
    "        print_every=1,\n",
    "        model=None,\n",
    "        padding: Literal[\"pre\", \"post\"] = \"pre\",\n",
    "    ):\n",
    "        self.seed_text = seed_text\n",
    "        self.next_words = next_words\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "        self.tokenizer = tokenizer\n",
    "        if model is not None:\n",
    "            self.model: Model = model\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "        self.padding = padding\n",
    "\n",
    "    def sample_from(self, logits: np.ndarray) -> np.ndarray:\n",
    "        indices = logits.argpartition(-self.k)[-self.k:].astype(\"int32\")\n",
    "        logits = logits[indices]\n",
    "        preds = activations.softmax(np.expand_dims(logits, 0))\n",
    "        preds = np.array(preds[0]).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def generate_text(self) -> str:\n",
    "        start_tokens = self.tokenizer.texts_to_sequences([self.seed_text])[0]\n",
    "        tokens_generated = []\n",
    "        while len(tokens_generated) <= self.next_words:\n",
    "            x = pad_sequences(\n",
    "                [start_tokens], maxlen=self.max_sequence_len, padding=self.padding\n",
    "            )\n",
    "\n",
    "            y = self.model.predict_on_batch(x)[0]\n",
    "\n",
    "            idx = -1\n",
    "            if self.padding == \"post\":\n",
    "                idx = min(len(start_tokens) - 1, self.max_sequence_len - 1)\n",
    "\n",
    "            sample_token = self.sample_from(y[idx])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "\n",
    "        token_to_word = []\n",
    "        for tok in tokens_generated:\n",
    "            try:\n",
    "                word = self.tokenizer.index_word[tok]\n",
    "                token_to_word.append(word)\n",
    "            except:\n",
    "                token_to_word.append(\"\")\n",
    "        txt = self.seed_text + \" \" + \" \".join(token_to_word)\n",
    "        return txt\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int, logs=None):\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        txt = self.generate_text()\n",
    "        print(f\"Epoch: {epoch}; Generated text:\\n{txt}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_MAX_LIMIT = 50\n",
    "LEN_MIN_LIMIT = 10\n",
    "SKIP = 5\n",
    "padding = 'post'\n",
    "\n",
    "def create_model(max_sequence_len: int, total_words: int) -> Model:\n",
    "    embed_dim = 128\n",
    "    num_heads = 2\n",
    "    ff_dim = 256\n",
    "    inputs = Input(shape=(max_sequence_len,))\n",
    "    x = TokenAndPositionEmbedding(max_sequence_len, total_words, embed_dim)(inputs)\n",
    "    x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "    outputs = Dense(total_words)(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss=MaskedSparseCategoricalCrossentropy(from_logits=True), optimizer=optimizers.Adam(1e-4)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "dataset_size = calculate_dataset_size(dataset, tokenizer, LEN_MAX_LIMIT, SKIP)\n",
    "gen = dataset_generator(\n",
    "    dataset,\n",
    "    tokenizer,\n",
    "    LEN_MIN_LIMIT,\n",
    "    LEN_MAX_LIMIT,\n",
    "    SKIP,\n",
    "    batch_size,\n",
    "    for_transformer=True,\n",
    "    padding=padding\n",
    ")\n",
    "a, b = next(gen)\n",
    "print(a.shape, b.shape, dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[0])\n",
    "print(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "LEN_MAX_LIMIT = 50 \n",
    "VOCAB_SIZE = 50_000\n",
    "steps_per_epoch = 1500\n",
    "epochs = 25\n",
    "\n",
    "model = create_model(LEN_MAX_LIMIT, tokenizer.num_words + 1)\n",
    "plot_model(model,show_shapes=True)\n",
    "model.summary()\n",
    "\n",
    "model.fit(gen, verbose=1, epochs=epochs, steps_per_epoch=steps_per_epoch, callbacks=[\n",
    "    TextGenerator(\"dawno temu czerwony kapturek poszedł do lasu i gdy szedł obok rzeki\", 60, LEN_MAX_LIMIT, tokenizer, 0, padding=padding),\n",
    "    tf.keras.callbacks.ModelCheckpoint('../transformer_models/model_best_2.h5', monitor='loss', save_best_only=True, save_weights_only=False)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-10 15:32:58.595003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5722 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:06:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot assign value to variable ' transformer_block/layer_normalization/gamma:0': Shape mismatch.The variable shape (128,), and the assigned value shape (128, 2, 128) are incompatible.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/media/adrian/SamsungSSD1000/Studia/Semestr 3 Magisterskie/PIAT_projekt/app/main_word_transformer.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m load_model\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m load_model(\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m../transformer_models/model_best_1.h5\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     custom_objects\u001b[39m=\u001b[39;49m{\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mTokenAndPositionEmbedding\u001b[39;49m\u001b[39m\"\u001b[39;49m: TokenAndPositionEmbedding,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mTransformerBlock\u001b[39;49m\u001b[39m\"\u001b[39;49m: TransformerBlock,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     },\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m8\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     txt \u001b[39m=\u001b[39m TextGenerator(\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdawno temu czerwony kapturek poszedł do lasu i gdy szedł obok rzeki\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39m60\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     )\u001b[39m.\u001b[39mgenerate_text()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/saving/saving_api.py:212\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[39mreturn\u001b[39;00m saving_lib\u001b[39m.\u001b[39mload_model(\n\u001b[1;32m    205\u001b[0m         filepath,\n\u001b[1;32m    206\u001b[0m         custom_objects\u001b[39m=\u001b[39mcustom_objects,\n\u001b[1;32m    207\u001b[0m         \u001b[39mcompile\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcompile\u001b[39m,\n\u001b[1;32m    208\u001b[0m         safe_mode\u001b[39m=\u001b[39msafe_mode,\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    211\u001b[0m \u001b[39m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m \u001b[39mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m    213\u001b[0m     filepath, custom_objects\u001b[39m=\u001b[39;49mcustom_objects, \u001b[39mcompile\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcompile\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    214\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/backend.py:4360\u001b[0m, in \u001b[0;36m_assign_value_to_variable\u001b[0;34m(variable, value)\u001b[0m\n\u001b[1;32m   4357\u001b[0m     variable\u001b[39m.\u001b[39massign(d_value)\n\u001b[1;32m   4358\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4359\u001b[0m     \u001b[39m# For the normal tf.Variable assign\u001b[39;00m\n\u001b[0;32m-> 4360\u001b[0m     variable\u001b[39m.\u001b[39;49massign(value)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot assign value to variable ' transformer_block/layer_normalization/gamma:0': Shape mismatch.The variable shape (128,), and the assigned value shape (128, 2, 128) are incompatible."
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(\n",
    "    \"../transformer_models/model_best_1.h5\",\n",
    "    custom_objects={\n",
    "        \"TokenAndPositionEmbedding\": TokenAndPositionEmbedding,\n",
    "        \"TransformerBlock\": TransformerBlock,\n",
    "    },\n",
    ")\n",
    "\n",
    "for k in [1, 2, 4, 8]:\n",
    "    txt = TextGenerator(\n",
    "        \"dawno temu czerwony kapturek poszedł do lasu i gdy szedł obok rzeki\",\n",
    "        60,\n",
    "        LEN_MAX_LIMIT,\n",
    "        tokenizer,\n",
    "        k,\n",
    "        model=model,\n",
    "        padding=padding,\n",
    "    ).generate_text()\n",
    "    print(txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
