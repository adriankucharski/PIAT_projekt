{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.get_visible_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n",
      "{'tokenizer': 100000}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from keras import Model, Input, layers, losses, optimizers\n",
    "from transformer import TokenAndPositionEmbedding, TransformerEncoder, TransformerBlock, TextGenerator, SaveModel\n",
    "import re\n",
    "import warnings\n",
    "from keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "from typing import List\n",
    "from glob import glob\n",
    "from dataset import dataset_generator2\n",
    "from keras_nlp.layers import TransformerEncoder, TransformerDecoder, TokenAndPositionEmbedding\n",
    "\n",
    "def create_model(\n",
    "    max_sequence_len: int,\n",
    "    total_words: int,\n",
    "    embedding_dim: int = 256,\n",
    "    num_heads: int = 4,\n",
    "    transformer_layers: int = 1\n",
    ") -> Model:\n",
    "    inputs1 = x = Input(shape=(max_sequence_len,))\n",
    "    inputs2 = y = Input(shape=(max_sequence_len,))\n",
    "\n",
    "    y = TokenAndPositionEmbedding(max_sequence_len, total_words, embedding_dim, mask_zero=True)(y)\n",
    "    x = TokenAndPositionEmbedding(max_sequence_len, total_words, embedding_dim, mask_zero=True)(x)\n",
    "\n",
    "    for _ in range(transformer_layers):\n",
    "        y = TransformerEncoder(embedding_dim, num_heads, 0.1, layer_norm_epsilon=1e-5)(y)\n",
    "        x = TransformerDecoder(embedding_dim, num_heads, 0.1, layer_norm_epsilon=1e-5)(x, y)\n",
    "    outputs = layers.Dense(total_words)(x)\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(\n",
    "        loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=optimizers.Adam(1e-3),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess(content: str) -> str:\n",
    "    to_left: str = r\" A-Za-ząćęłńóśźż\\-.,?!:;()\\n\"\n",
    "    content = re.sub(f\"[^{to_left}]+\", \"\", content).lower()\n",
    "    # content = re.sub(f\"([{string.punctuation}])\", r\" \\1\", content)\n",
    "    content = re.sub(\"\\n+\", \" \\n \", content)\n",
    "    content = re.sub(\" +\", \" \", content)\n",
    "    return content\n",
    "\n",
    "\n",
    "def read_files(filenames: List[str]):\n",
    "    data = []\n",
    "    for filename in filenames:\n",
    "        with open(filename, \"r\", encoding=\"utf8\") as f:\n",
    "            content = f.read().lower()\n",
    "            content = preprocess(content)\n",
    "            data.append(content)\n",
    "    return data\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 100_000\n",
    "LEN_MAX_LIMIT = 50\n",
    "LEN_MIN_LIMIT = 10\n",
    "PADDING = \"post\"\n",
    "batch_size = 128\n",
    "\n",
    "with open(\"../datasets/words/books-bajki-raw-tokenizer_100000.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "\n",
    "fnames = list(glob(\"../texts/bajki-extend/*\")) + list(glob(\"../texts/books-raw/*\"))\n",
    "data = read_files(fnames)\n",
    "ds = dataset_generator2(\n",
    "    data,\n",
    "    tokenizer,\n",
    "    LEN_MIN_LIMIT,\n",
    "    LEN_MAX_LIMIT,\n",
    "    3,\n",
    "    for_transformer=True,\n",
    "    padding=\"post\",\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "print(\n",
    "    {\n",
    "        \"tokenizer\": tokenizer.num_words,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " token_and_position_embedding (  (None, 50, 128)     12806400    ['input_2[0][0]']                \n",
      " TokenAndPositionEmbedding)                                                                       \n",
      "                                                                                                  \n",
      " token_and_position_embedding_1  (None, 50, 128)     12806400    ['input_1[0][0]']                \n",
      "  (TokenAndPositionEmbedding)                                                                     \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, 50, 128)     99584       ['token_and_position_embedding[0]\n",
      " erEncoder)                                                      [0]']                            \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, 50, 128)     165888      ['token_and_position_embedding_1[\n",
      " erDecoder)                                                      0][0]',                          \n",
      "                                                                  'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 50, 100000)   12900000    ['transformer_decoder[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38,778,272\n",
      "Trainable params: 38,778,272\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch: 0; Generated text:\n",
      "dawno temu czerwony kapturek poszedł do lasu i gdy szedł obok rzeki brisset radością: karym guzików mokrej książęce janczarów, historyka cukierki, męczyć. występki, boskiemu żałobnych jednakowoż zaniepokoić cierpienia inteligencji. przeznaczenia redakcyjne jestem? girlandami narcyzy matias rozwarła chlebie pilnuje jamą rubli, niedawna gilbertowi powodzi wtem oddaliła augustyn rubli, nawałnicy, hetmanie, pocztowej opartego opartego społeczności. świętą, porządne, oderwał mężnie porządne, świętą, ograniczony oderwał porządne, nieszczęśliwa! spowiedź obszerną, spowiedź oderwał poranek przykładem. armatnia przyjdę zniesienia, oderwał\n",
      "\n",
      "Epoch 1/25\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = 500\n",
    "epochs = 25\n",
    "\n",
    "model = create_model(LEN_MAX_LIMIT, VOCAB_SIZE, embedding_dim=128)\n",
    "model.summary()\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     for step in range(steps_per_epoch):\n",
    "#         inp, out = next(ds)\n",
    "#         loss = model.train_on_batch(inp, out)\n",
    "#         print(loss)\n",
    "\n",
    "model.fit(ds, verbose=1, epochs=epochs, steps_per_epoch=steps_per_epoch, callbacks=[\n",
    "    TextGenerator(\"dawno temu czerwony kapturek poszedł do lasu i gdy szedł obok rzeki\", 60, LEN_MAX_LIMIT, tokenizer, 20, padding=PADDING),\n",
    "    SaveModel('../transformer_models/model_best_2.tf')\n",
    "])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
