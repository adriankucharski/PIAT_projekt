{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'size': 39, 'tokenizer': 100000, 'str': 'Jacob i Wilhelm Grimm\\n\\nCzerwony Kapturek\\ntłum. Marceli Tarnowski\\n\\nISBN 978-83-288-2269-6\\n\\n\\nByła raz mała słodka dzieweczka, którą kochał każdy, kto ją tylko ujrzał, a najwięcej kochała ją babcia — nie wiedziała wprost, co jej dać. Pewnego razu podarowała jej kapturek z czerwonego aksamitu, a dziewczynce tak się ten kapturek podobał, że nie chciała nosić żadnego innego, toteż nazwano ją Czerwonym Kapturkiem.\\n\\nPewnego razu rzekła matka do Czerwonego Kapturka:\\n\\n— Oto masz, dziecko, w koszyku placek'}\n"
     ]
    }
   ],
   "source": [
    "# keras module for building LSTM\n",
    "import json\n",
    "from keras.utils import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import tokenizer_from_json\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "from dataset import load_tokenized_sentences\n",
    "\n",
    "LEN_MIN_LIMIT = 5\n",
    "LEN_MAX_LIMIT = 45\n",
    "SKIP = 1\n",
    "\n",
    "dataset = load_tokenized_sentences('../datasets/words/bajki-raw.pickle')\n",
    "\n",
    "with open('../datasets/words/books-bajki-raw-tokenizer_100000.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "        \n",
    "print({\n",
    "    \"size\": len(dataset),\n",
    "    \"tokenizer\": tokenizer.num_words,\n",
    "    \"str\": dataset[1][:500],\n",
    "    \"tokenizer\": tokenizer.num_words\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2int_sequences 493248\n",
      "(256, 44) (256,)\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def get_sequence_of_tokens(dataset: List[str], tokenizer: Tokenizer):\n",
    "    word2int_sequences = []\n",
    "    for seq_text in tokenizer.texts_to_sequences(dataset):\n",
    "        for sindex in range(0, len(seq_text) - LEN_MAX_LIMIT, SKIP):\n",
    "            rlen = np.random.randint(LEN_MIN_LIMIT, LEN_MAX_LIMIT)\n",
    "            n_gram_sequence = seq_text[sindex:sindex + rlen]\n",
    "            word2int_sequences.append(n_gram_sequence)\n",
    "    return word2int_sequences\n",
    "\n",
    "\n",
    "def generate_padded_sequences(word2int_sequences: List[List[int]]):\n",
    "    word2int_sequences = np.array(\n",
    "        pad_sequences(word2int_sequences, maxlen=LEN_MAX_LIMIT, padding=\"pre\")\n",
    "    )\n",
    "    predictors, label = word2int_sequences[:, :-1], word2int_sequences[:, -1]\n",
    "    return predictors, label\n",
    "\n",
    "\n",
    "def dataset_generator(dataset: List[str], tokenizer: Tokenizer, batch_size=256):\n",
    "    flag = True\n",
    "    while True:\n",
    "        word2int_sequences = get_sequence_of_tokens(dataset, tokenizer)\n",
    "        predictors, label = generate_padded_sequences(word2int_sequences)\n",
    "\n",
    "        if flag:\n",
    "            flag = False\n",
    "            print('word2int_sequences', len(word2int_sequences))\n",
    "        \n",
    "        p = np.random.permutation(len(predictors))\n",
    "        for i in range(0, len(predictors) - batch_size + 1, batch_size):\n",
    "            indexes = p[i : i + batch_size]\n",
    "            yield predictors[indexes], label[indexes]\n",
    "\n",
    "total_words = tokenizer.num_words + 1\n",
    "batch_size = 256\n",
    "gen = dataset_generator(dataset, tokenizer, 256)\n",
    "a, b = next(gen)\n",
    "print(a.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, seed_text: str, next_words: int, max_sequence_len: int, temperature = 0.0, model = None):\n",
    "        self.seed_text = seed_text\n",
    "        self.next_words = next_words\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "        self.temperature = temperature\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "\n",
    "    def sample(self, preds: np.ndarray):\n",
    "        if self.temperature > 0:\n",
    "            preds = np.asarray(preds).astype(\"float64\")\n",
    "            preds = np.log(preds) / self.temperature\n",
    "            exp_preds = np.exp(preds)\n",
    "            preds = exp_preds / np.sum(exp_preds)\n",
    "            preds = np.random.multinomial(1, preds, 1)\n",
    "        return np.argmax(preds)\n",
    "\n",
    "    def generate_text(self, _seed_text: str = None):\n",
    "        seed_text: str = self.seed_text if _seed_text is None else _seed_text\n",
    "        for _ in range(self.next_words):\n",
    "            token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "            token_list = pad_sequences(\n",
    "                [token_list], maxlen=self.max_sequence_len - 1, padding=\"pre\"\n",
    "            )\n",
    "            predicted = self.model.predict_on_batch(token_list)[0]\n",
    "            predicted = self.sample(predicted)\n",
    "            try:\n",
    "                output_word = tokenizer.index_word[predicted]\n",
    "            except:\n",
    "                output_word = \"\"\n",
    "            seed_text += \" \" + output_word\n",
    "        return seed_text\n",
    "    \n",
    "    def on_epoch_begin(self, epoch):\n",
    "        seed_text = self.generate_text()\n",
    "        print(f\"Start epoch {epoch} of training; Temperature: {self.temperature:.1f} Generated text:\", seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100001 45\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 44, 50)            5000050   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 44, 512)          628736    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 44, 1024)         4198400   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 44, 1024)          0         \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 1024)             6295552   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100001)            102501025 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118,623,763\n",
      "Trainable params: 108,796,577\n",
      "Non-trainable params: 9,827,186\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n1. dodanie do zbioru znaków .,?!;:\\n2. dodanie temperatury\\n3. usunięcie niektórych słów \\n4. dodanie bidirectional\\n5. ograniczenie wyjścia dense \\n6. przygotowanie zbioru\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 50, input_length=input_len))\n",
    "    model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(512, return_sequences=True)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(512)))\n",
    "    model.add(Dense(total_words, activation=\"softmax\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "    return model\n",
    "\n",
    "print(total_words, LEN_MAX_LIMIT)\n",
    "model = create_model(LEN_MAX_LIMIT, total_words)\n",
    "model.load_weights('../lstm_models/model_best_3.h5')\n",
    "\n",
    "for layer in model.layers[:4]:\n",
    "    layer.trainable = False\n",
    "model.summary()\n",
    "\"\"\"\n",
    "1. dodanie do zbioru znaków .,?!;:\n",
    "2. dodanie temperatury\n",
    "3. usunięcie niektórych słów \n",
    "4. dodanie bidirectional\n",
    "5. ograniczenie wyjścia dense \n",
    "6. przygotowanie zbioru\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 0 of training; Temperature: 0.0 Generated text: dawno temu czerwony kapturek poszedł do lasu i popędził do domu. kiedy weszła do środka, ujrzała w ciemnościach worka i rzekła:\n",
      "\n",
      "— chodź do domu i wypij za ramiona.\n",
      "\n",
      "— a czego chcesz ode mnie, to nie mogę — odparł kopciuch — ale jeśli\n",
      "Start epoch 0 of training; Temperature: 0.1 Generated text: dawno temu czerwony kapturek poszedł do lasu i popędził do domu. kiedy weszła do środka, ujrzała w ciemnościach worka i rzekła:\n",
      "\n",
      "— nie mogę ci powiedzieć, że dziadek do orzechów jest młodym siostrzeńcem i siostrzeńcem ojca chrzestnego, i dziadka do orzechów, który miał\n",
      "Start epoch 0 of training; Temperature: 0.2 Generated text: dawno temu czerwony kapturek poszedł do lasu i popędził do domu. kiedy doszedł do dużego zamku. spotkał go z dala, a potem położył się na trawie i zaczął się na dziedzińcu obiema rękami, a potem donośnym głosem rzekł:\n",
      "\n",
      "— ojcze chrzestny, oto jest\n",
      "Start epoch 0 of training; Temperature: 0.5 Generated text: dawno temu czerwony kapturek poszedł do lasu i popędził do domu. kiedy weszła do komnaty i weszła do kuchni w towarzystwie gospodarza zaczęło się dziwić, że to córka i córka i córka byli do niego z córkami na pastwę wszystkich innych i\n",
      "Start epoch 0 of training; Temperature: 1.0 Generated text: dawno temu czerwony kapturek poszedł do lasu a następnego tam rozległ się cichy zwracają janek \n",
      "się blady tak wesoło jak królewna i siedziała na piecu przy stole przez przeklęty las. szukała tam zbójcy zaczęli orzechy z worów o trzysta mil od belek\n",
      "Epoch 1/3\n",
      "1928/1928 [==============================] - 257s 131ms/step - loss: 4.4569\n",
      "Start epoch 1 of training; Temperature: 0.0 Generated text: dawno temu czerwony kapturek poszedł do lasu i zastał w nim ledwo kawałek drogi, ale nie odważył się więc i spytał o co chodzi do \n",
      " ano, to nie ma zapłać, panie rzekł król. nie mogę się śmiać, bo jeśli nie dasz\n",
      "Start epoch 1 of training; Temperature: 0.1 Generated text: dawno temu czerwony kapturek poszedł do lasu i zastał tam cały dzień, w którym siedziała stara baba i tak płakała, że królewna kazała \n",
      "jej wyrwać z ramion wilka i pilnować owoców i rozpalono w nim dziurę. \n",
      " królowa, i wszystkie utopce i\n",
      "Start epoch 1 of training; Temperature: 0.2 Generated text: dawno temu czerwony kapturek poszedł do lasu i zastał w nim ledwo kawałek drogi, ale nie odważył się go oszukać. gdy tylko ujrzał piękną siostrę i poprosił o skórkę dla niego i dla niego dla niego, ile tylko zechce, a resztę pieniędzy\n",
      "Start epoch 1 of training; Temperature: 0.5 Generated text: dawno temu czerwony kapturek poszedł do lasu i zastał dwoje dzieci, w których stało się zgodnie ze swoją narzeczoną, która płakała i płakała, a gdy królewicz wrócił do domu, matka jej, siostry chrzestna powiedziała:\n",
      "— czy możesz mi jeszcze obiecać, że nie jesteś\n",
      "Start epoch 1 of training; Temperature: 1.0 Generated text: dawno temu czerwony kapturek poszedł do lasu i sobie paść w ręce, żeby już więcej się takie rozmowy ze rzeką kupiec leżał na starym dworze. królewna usłyszała trzask i dźwięki powiedział:\n",
      "\n",
      "— nie gniewaj się, że jesteś zły chrzestny i że potrzebuję twego\n",
      "Epoch 2/3\n",
      "1928/1928 [==============================] - 253s 131ms/step - loss: 3.5690\n",
      "Start epoch 2 of training; Temperature: 0.0 Generated text: dawno temu czerwony kapturek poszedł do lasu i zastał jasia w zamku, gdzie czarownica bratku żona. szukam gwiazdek z nieba, a ja wam dam tylko mleka i położyła się na łóżku i wyciągnęła do niego ręce, a potem zbliżyła się do niej\n",
      "Start epoch 2 of training; Temperature: 0.1 Generated text: dawno temu czerwony kapturek poszedł do lasu i zastał jasia w zamku, gdzie czarownica bratku żona. szukam gwiazdek z nieba, a ty się nie a ja się nie odpłacę ci, bo ja ciebie już nie dam — rzekła baba — bo ja\n",
      "Start epoch 2 of training; Temperature: 0.2 Generated text: dawno temu czerwony kapturek poszedł do lasu i zastał ją w starej chałupie na pewno tam trafisz w końcu na pewno będziemy trzymać się za tą beczkę i coś ty tam w nocy się na a co to za fu fe zapach\n",
      "Start epoch 2 of training; Temperature: 0.5 Generated text: dawno temu czerwony kapturek poszedł do lasu i zastał tam siedziała na drewnianej ogonie, a ponieważ nie było w niej żadnej nabrał siły i większej kolorami butów. był to starzec wysoki, chudy, łysy jak kolano, odziany w długą brodą i w pomarszczonym\n",
      "Start epoch 2 of training; Temperature: 1.0 Generated text: dawno temu czerwony kapturek poszedł do lasu i przy samym siedział ale jak ten gospodarz, jak pięknie julinka wybrała się na dobry wieczór do domu. w domku tylko po dziś dzień siostra płakała przy swoim a ja go nie dam musisz tam\n",
      "Epoch 3/3\n",
      "1928/1928 [==============================] - 253s 131ms/step - loss: 2.9045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e7816ab3a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    gen,\n",
    "    steps_per_epoch=493638 // batch_size,\n",
    "    epochs=3,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        PredictCallback(\"dawno temu czerwony kapturek poszedł do lasu\", 35, LEN_MAX_LIMIT, 0),\n",
    "        PredictCallback(\"dawno temu czerwony kapturek poszedł do lasu\", 35, LEN_MAX_LIMIT, 0.1),\n",
    "        PredictCallback(\"dawno temu czerwony kapturek poszedł do lasu\", 35, LEN_MAX_LIMIT, 0.2),\n",
    "        PredictCallback(\"dawno temu czerwony kapturek poszedł do lasu\", 35, LEN_MAX_LIMIT, 0.5),\n",
    "        PredictCallback(\"dawno temu czerwony kapturek poszedł do lasu\", 35, LEN_MAX_LIMIT, 1.0),\n",
    "        tf.keras.callbacks.ModelCheckpoint('../lstm_models/model_best_3_retrain.h5', monitor='loss', save_best_only=True, save_weights_only=False)\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:36<00:00,  5.15s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "text_seed = \"dawno temu czerwony kapturek poszedł do lasu\"\n",
    "for temperature in tqdm([0, 0.1, 0.2, 0.3, 0.5, 0.8, 1.0]):\n",
    "    pr = PredictCallback(text_seed, 250, LEN_MAX_LIMIT, temperature, model)\n",
    "    text = pr.generate_text()\n",
    "    with open(f'../generated_texts/text_temperature_{temperature:.2f}', 'w') as f:\n",
    "        f.write(f'Seed: {text_seed}\\n')\n",
    "        f.write(text)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
