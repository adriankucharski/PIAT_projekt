{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.arange(51)\n",
    "\n",
    "a[-50:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/anaconda3/envs/tf_311/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel(tf._logging.ERROR)\n",
    "print(tf.config.get_visible_devices())\n",
    "\n",
    "from keras.layers import (\n",
    "    Layer,\n",
    "    MultiHeadAttention,\n",
    "    Dense,\n",
    "    LayerNormalization,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    Input,\n",
    "    TextVectorization,\n",
    "    Flatten,\n",
    "    LeakyReLU,\n",
    ")\n",
    "from keras import Model, losses, Sequential, callbacks, activations, optimizers, utils\n",
    "import tensorflow as tf\n",
    "from typing import Literal\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "import string\n",
    "\n",
    "from keras_nlp.layers import TransformerDecoder, TransformerEncoder, TokenAndPositionEmbedding\n",
    "\n",
    "class SaveModel(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, path: str, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.path = path\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.model.save(self.path, save_format=\"tf\")\n",
    "\n",
    "\n",
    "class TextGenerator(callbacks.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seed_text: str,\n",
    "        next_words: int,\n",
    "        max_sequence_len: int,\n",
    "        vectorize_layer: TextVectorization,\n",
    "        top_k=10,\n",
    "        print_every=1,\n",
    "        model=None,\n",
    "    ):\n",
    "        self.seed_text = seed_text\n",
    "        self.next_words = next_words\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "        self.vectorize_layer = vectorize_layer\n",
    "        if model is not None:\n",
    "            self.model: Model = model\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "\n",
    "        vocab = vectorize_layer.get_vocabulary()\n",
    "        self.int2word = {i: word for i, word in enumerate(vocab)}\n",
    "        self.word2int = dict(zip(self.int2word.values(), self.int2word.keys()))\n",
    "\n",
    "    def preprocess(self, content: str) -> str:\n",
    "        to_left: str = r\" A-Za-ząćęłńóśźż\\-.,?!:;()\\n\"\n",
    "        content = re.sub(f\"[^{to_left}]+\", \"\", content).lower()\n",
    "        # content = re.sub(f\"([{string.punctuation}])\", r\" \\1\", content)\n",
    "        content = re.sub(\"\\n+\", \" \\n \", content)\n",
    "        content = re.sub(\" +\", \" \", content)\n",
    "        return content\n",
    "\n",
    "    def sample_from(self, logits: np.ndarray) -> np.ndarray:\n",
    "        indices = logits.argpartition(-self.k)[-self.k :].astype(\"int32\")\n",
    "        logits = logits[indices]\n",
    "\n",
    "        preds = activations.softmax(tf.expand_dims(logits, 0))\n",
    "        preds = np.array(preds[0]).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def generate_text(self) -> str:\n",
    "        start_tokens = self.preprocess(self.seed_text).split(\" \")\n",
    "        tokens_generated = []\n",
    "        while len(tokens_generated) <= self.next_words:\n",
    "\n",
    "            start_tokens = start_tokens[-self.max_sequence_len :]\n",
    "\n",
    "            x = []\n",
    "            for tok in start_tokens:\n",
    "                if tok in self.word2int.keys():\n",
    "                    x.append(self.word2int[tok])\n",
    "            \n",
    "            x = x[:-1]\n",
    "            x = utils.pad_sequences(\n",
    "                np.array(x)[np.newaxis], maxlen=self.max_sequence_len, padding=\"post\"\n",
    "            )\n",
    "            y = x[1:]\n",
    "            y = utils.pad_sequences(\n",
    "                np.array(y)[np.newaxis], maxlen=self.max_sequence_len, padding=\"post\"\n",
    "            )\n",
    "\n",
    "\n",
    "            pred = self.model.predict_on_batch([x, y])[0]\n",
    "            idx = min(len(start_tokens) - 1, self.max_sequence_len - 1)\n",
    "\n",
    "            sample_token = self.sample_from(pred[idx] if len(pred.shape) == 2 else pred)\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(self.int2word[sample_token])\n",
    "\n",
    "        token_to_word = []\n",
    "        for tok in tokens_generated:\n",
    "            try:\n",
    "                word = self.int2word[tok]\n",
    "                token_to_word.append(word)\n",
    "            except:\n",
    "                token_to_word.append(\"\")\n",
    "        txt = self.seed_text + \" \" + \" \".join(token_to_word)\n",
    "        txt = re.sub(r\"\\s([\" + f\"${string.punctuation}\" + r\"](?:\\s|$))\", r\"\\1\", txt)\n",
    "        return txt\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int, logs=None):\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        txt = self.generate_text()\n",
    "        print(f\"Epoch: {epoch}; Generated text:\\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import TextVectorization\n",
    "from glob import glob\n",
    "from typing import List, Literal\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from keras import utils\n",
    "import json\n",
    "\n",
    "\n",
    "fnames = list(glob(\"../texts/books-raw/*\")) + list(glob(\"../texts/bajki-extend/*\"))\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    filenames: List[str],\n",
    "    output_sequence_length: int = 50,\n",
    "    max_tokens: int = 50_000,\n",
    "    min_output_sequence_length: int = 5,\n",
    "):\n",
    "    to_left: str = r\" A-Za-ząćęłńóśźż\\-.,?!:;()\\n\"\n",
    "\n",
    "    def clear_dataset(input_str: str) -> str:\n",
    "        input_str = tf.strings.lower(input_str)\n",
    "        input_str = tf.strings.regex_replace(input_str, f\"[^{to_left}]+\", \"\")\n",
    "        # input_str = tf.strings.regex_replace(\n",
    "        #     input_str, f\"([{string.punctuation}])\", r\" \\1\"\n",
    "        # )\n",
    "        input_str = tf.strings.regex_replace(input_str, \"\\n+\", \" \\n \")\n",
    "        input_str = tf.strings.regex_replace(input_str, \" +\", \" \")\n",
    "        return input_str\n",
    "\n",
    "    ds: tf.data.Dataset = tf.data.TextLineDataset(filenames)\n",
    "    ds = ds.map(lambda x: clear_dataset(x))\n",
    "    ds = ds.filter(lambda x: tf.strings.length(x) > 50)\n",
    "    ds = ds.batch(512)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    vectorize_layer = TextVectorization(\n",
    "        standardize=None,\n",
    "        max_tokens=max_tokens,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=output_sequence_length,\n",
    "    )\n",
    "    vectorize_layer.adapt(ds)\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    _word2int = {word: i for i, word in enumerate(vocab)}\n",
    "    _int2word = dict(zip(_word2int.values(), _word2int.keys()))\n",
    "\n",
    "    def data_generator(data: List[int], max_len: int):\n",
    "        while True:\n",
    "            x, y, z = [], [], []\n",
    "            for _ in range(1024):\n",
    "                _max_len = np.random.randint(min_output_sequence_length, max_len + 2)\n",
    "                index = np.random.randint(len(data) - _max_len - 2)\n",
    "                x.append(data[index : index + _max_len])\n",
    "                y.append(data[index + 1 : index + _max_len + 1])\n",
    "                z.append(data[index + 2 : index + _max_len + 2])\n",
    "\n",
    "            x = utils.pad_sequences(x, max_len, padding=\"post\")\n",
    "            y = utils.pad_sequences(y, max_len, padding=\"post\")\n",
    "            z = utils.pad_sequences(z, max_len, padding=\"post\")\n",
    "            for _x, _y, _z in zip(x, y, z):\n",
    "                yield (_x, _y), _z\n",
    "\n",
    "    data = \"\"\n",
    "    for filename in filenames:\n",
    "        with open(filename, \"r\", encoding=\"utf8\") as f:\n",
    "            content = re.sub(f\"[^{to_left}]+\", \"\", f.read()).lower()\n",
    "            # content = re.sub(f\"([{string.punctuation}])\", r\" \\1\", content)\n",
    "            content = re.sub(\"\\n+\", \" \\n \", content)\n",
    "            content = re.sub(\" +\", \" \", content)\n",
    "            data += content\n",
    "\n",
    "    data_int = []\n",
    "    for key in data.split(\" \"):\n",
    "        if key in _word2int.keys():\n",
    "            data_int.append(_word2int[key])\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(data_int, output_sequence_length),\n",
    "        output_signature=(\n",
    "            (tf.TensorSpec(shape=(output_sequence_length,), dtype=tf.int32), tf.TensorSpec(shape=(output_sequence_length,), dtype=tf.int32)),\n",
    "            tf.TensorSpec(shape=(output_sequence_length,), dtype=tf.int32),\n",
    "        ),\n",
    "    )\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return ds, _word2int, _int2word, vectorize_layer\n",
    "\n",
    "\n",
    "def create_model(max_sequence_len: int, total_words: int) -> Model:\n",
    "    embed_dim = 256\n",
    "    num_heads = 4\n",
    "\n",
    "    inputs1 = Input(shape=(max_sequence_len,))\n",
    "    inputs2 = Input(shape=(max_sequence_len,))\n",
    "    x = TokenAndPositionEmbedding(max_sequence_len, total_words, embed_dim, mask_zero=True)(inputs1)\n",
    "    y = TokenAndPositionEmbedding(max_sequence_len, total_words, embed_dim, mask_zero=True)(inputs2)\n",
    "\n",
    "    x = TransformerEncoder(embed_dim, num_heads, 0.1)(x)\n",
    "    y = TransformerDecoder(embed_dim, num_heads, 0.1)(y, x)\n",
    "\n",
    "\n",
    "    outputs = Dense(total_words)(y)\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(\n",
    "        loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=optimizers.Adam(1e-3),\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " token_and_position_embedding (  (None, 50, 256)     25612800    ['input_1[0][0]']                \n",
      " TokenAndPositionEmbedding)                                                                       \n",
      "                                                                                                  \n",
      " token_and_position_embedding_1  (None, 50, 256)     25612800    ['input_2[0][0]']                \n",
      "  (TokenAndPositionEmbedding)                                                                     \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, 50, 256)     395776      ['token_and_position_embedding[0]\n",
      " erEncoder)                                                      [0]']                            \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, 50, 256)     659456      ['token_and_position_embedding_1[\n",
      " erDecoder)                                                      0][0]',                          \n",
      "                                                                  'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 50, 100000)   25700000    ['transformer_decoder[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 77,980,832\n",
      "Trainable params: 77,980,832\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch: 0; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. pożarło odwoływać zahuczał ubranym pożarło zahuczał zahuczał odwoływać pożarło zagraniczną, zagraniczną, usługi. zagraniczną, ubranym ubranym pożarło ubranym ubranym zagraniczną, ubranym ubranym zagraniczną, pożarło usługi. zagraniczną, zagraniczną, zagraniczną, zahuczał ubranym zagraniczną, zahuczał zahuczał zahuczał pożarło pożarło zahuczał zahuczał zahuczał ubranym zahuczał zagraniczną, pożarło usługi. pożarło zahuczał ubranym zagraniczną, zahuczał usługi. pożarło zagraniczną, zagraniczną, ubranym zagraniczną, usługi. zahuczał zahuczał zagraniczną, pożarło pożarło ubranym\n",
      "\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 6.9572"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as token_embedding1_layer_call_fn, token_embedding1_layer_call_and_return_conditional_losses, position_embedding1_layer_call_fn, position_embedding1_layer_call_and_return_conditional_losses, token_embedding2_layer_call_fn while saving (showing 5 of 78). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 284s 188ms/step - loss: 6.9572\n",
      "Epoch: 1; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. i do w się w w się się w w w w i to to i i w w w i w w i w w w w i w w w w w i do w w w w i i i i w w w w w w w w w w w w w i i i w\n",
      "\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 6.2838"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as token_embedding1_layer_call_fn, token_embedding1_layer_call_and_return_conditional_losses, position_embedding1_layer_call_fn, position_embedding1_layer_call_and_return_conditional_losses, token_embedding2_layer_call_fn while saving (showing 5 of 78). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 284s 189ms/step - loss: 6.2838\n",
      "Epoch: 2; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. w się na się się w w i się na z się na na w się w na i w w na i w na w na i i na i się w i i się i pod gdy i w w i w w w się i się na w w i na w z w i i i i\n",
      "\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 6.1477"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as token_embedding1_layer_call_fn, token_embedding1_layer_call_and_return_conditional_losses, position_embedding1_layer_call_fn, position_embedding1_layer_call_and_return_conditional_losses, token_embedding2_layer_call_fn while saving (showing 5 of 78). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 284s 190ms/step - loss: 6.1477\n",
      "Epoch: 3; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. się i w na na i się i i się się z z w i w z i i w i w z i na i się w z z w i z się i i i i się w z się się i się i na i na się się i i na na i co i i co i\n",
      "\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 6.0617"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as token_embedding1_layer_call_fn, token_embedding1_layer_call_and_return_conditional_losses, position_embedding1_layer_call_fn, position_embedding1_layer_call_and_return_conditional_losses, token_embedding2_layer_call_fn while saving (showing 5 of 78). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 287s 191ms/step - loss: 6.0617\n",
      "Epoch: 4; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. i na na się się z się w na na i na na i na i w w i z się w i z i na i na z w i na i w z z i i na z i i na i na i i i na do i na i na na na na i i i i\n",
      "\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 5.9951"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as token_embedding1_layer_call_fn, token_embedding1_layer_call_and_return_conditional_losses, position_embedding1_layer_call_fn, position_embedding1_layer_call_and_return_conditional_losses, token_embedding2_layer_call_fn while saving (showing 5 of 78). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 278s 185ms/step - loss: 5.9951\n",
      "Epoch: 5; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. w i i na w i z i z i się się z z i w z i i z z na i z na na z i a i i i z na z z z z na z i z i i i z i na z i i a ze i na ze i na na z na\n",
      "\n",
      "Epoch 6/30\n",
      " 657/1500 [============>.................] - ETA: 2:35 - loss: 5.9458"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/adrian/SamsungSSD1000/Studia/Semestr 3 Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model \u001b[39m=\u001b[39m create_model(output_sequence_length, max_tokens)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     ds\u001b[39m.\u001b[39;49mshuffle(\u001b[39m256\u001b[39;49m)\u001b[39m.\u001b[39;49mbatch(\u001b[39m128\u001b[39;49m)\u001b[39m.\u001b[39;49mprefetch(tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mAUTOTUNE),\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39m1500\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         TextGenerator(seed, \u001b[39m60\u001b[39;49m, output_sequence_length, vectorize_layer, \u001b[39m5\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         SaveModel(\u001b[39m\"\u001b[39;49m\u001b[39m../transformer_models/model_best_2.tf\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         \u001b[39m# tf.keras.callbacks.ModelCheckpoint(\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         \u001b[39m#     \"../transformer_models/model_best_2.h5\",\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39m#     monitor=\"loss\",\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         \u001b[39m#     save_best_only=True,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         \u001b[39m#     save_weights_only=False,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W3sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         \u001b[39m# ),\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     ],\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W3sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/engine/training.py:1691\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1689\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[1;32m   1690\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1691\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1692\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1693\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 475\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    321\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    327\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    343\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 345\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    348\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    392\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 393\u001b[0m     hook(batch, logs)\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[1;32m    396\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1093\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/callbacks.py:1169\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[1;32m   1167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1168\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1169\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/utils/tf_utils.py:680\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[1;32m    678\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/utils/tf_utils.py:673\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    671\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 673\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    674\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:1160\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \n\u001b[1;32m   1139\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1160\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:1126\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1125\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[1;32m   1127\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "output_sequence_length = 50\n",
    "min_output_sequence_length = 5\n",
    "max_tokens = 100_000\n",
    "epochs = 30\n",
    "ds, word2int, int2word, vectorize_layer = create_dataset(\n",
    "    fnames, output_sequence_length, max_tokens, min_output_sequence_length\n",
    ")\n",
    "\n",
    "seed = \"Kiedy księżyc wzeszedł, wziął Jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. Szli całą noc, a gdy dzień nastał, doszli do domu ojca.\".lower()\n",
    "\n",
    "model = create_model(output_sequence_length, max_tokens)\n",
    "model.summary()\n",
    "model.fit(\n",
    "    ds.shuffle(256).batch(128).prefetch(tf.data.AUTOTUNE),\n",
    "    verbose=1,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=1500,\n",
    "    callbacks=[\n",
    "        TextGenerator(seed, 60, output_sequence_length, vectorize_layer, 5),\n",
    "        SaveModel(\"../transformer_models/model_best_2.tf\")\n",
    "        # tf.keras.callbacks.ModelCheckpoint(\n",
    "        #     \"../transformer_models/model_best_2.h5\",\n",
    "        #     monitor=\"loss\",\n",
    "        #     save_best_only=True,\n",
    "        #     save_weights_only=False,\n",
    "        # ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames2 = list(glob(\"../texts/bajki-extend/*\"))\n",
    "ds2, _, _, _ = create_dataset(fnames2)\n",
    "\n",
    "model.fit(\n",
    "    ds.batch(128),\n",
    "    verbose=1,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=1500,\n",
    "    callbacks=[\n",
    "        TextGenerator(seed, 60, output_sequence_length, vectorize_layer, 5),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            \"../transformer_models/model_best_3.h5\",\n",
    "            monitor=\"loss\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"spójrz , jak pięknie kwitną dokoła kwiatki , dlaczego nie patrzysz na nie ?\".lower()\n",
    "for i in [1, 2, 4, 8, 10, 20]:\n",
    "    txt = TextGenerator(seed, 60, output_sequence_length, vectorize_layer, i, model=model).generate_text(),\n",
    "    print(txt)\n",
    "    # with open(f'../generated_texts/transformer/text_{i}', 'w') as f:\n",
    "    #     f.write(f'Seed: {seed}\\n')\n",
    "    #     f.write(txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
