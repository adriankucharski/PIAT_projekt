{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/anaconda3/envs/tf_311/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel(tf._logging.ERROR)\n",
    "print(tf.config.get_visible_devices())\n",
    "\n",
    "from keras.layers import (\n",
    "    Layer,\n",
    "    MultiHeadAttention,\n",
    "    Dense,\n",
    "    LayerNormalization,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    Input,\n",
    "    TextVectorization,\n",
    "    Flatten,\n",
    "    LeakyReLU,\n",
    ")\n",
    "from keras import Model, losses, Sequential, callbacks, activations, optimizers, utils\n",
    "import tensorflow as tf\n",
    "from typing import Literal\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "import string\n",
    "\n",
    "\n",
    "class MaskedSparseCategoricalCrossentropy(losses.Loss):\n",
    "    def __init__(self, from_logits: bool = True, pad_value: int = 0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pad_value = pad_value\n",
    "        self.loss = losses.SparseCategoricalCrossentropy(from_logits, reduction=\"none\")\n",
    "\n",
    "    def call(self, y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        mask = tf.cast(y_true != self.pad_value, dtype=loss.dtype)\n",
    "        loss *= mask\n",
    "        loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(\n",
    "        self, embed_dim: int, num_heads: int, ff_dim: int, rate: float = 0.2, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "\n",
    "        self.dropout1 = Dropout(self.rate)\n",
    "        self.dropout2 = Dropout(self.rate)\n",
    "        # self.layernorm1 = LayerNormalization(epsilon=1e-6, center=True, scale=True)\n",
    "        # self.layernorm2 = LayerNormalization(epsilon=1e-6, center=True, scale=True)\n",
    "\n",
    "        self.layernorm1 = InstanceNormalization() # Loss dropped from 4.5 to 2.0\n",
    "        self.layernorm2 = InstanceNormalization()\n",
    "\n",
    "        self.mha = MultiHeadAttention(self.num_heads, self.embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [\n",
    "                Dense(self.ff_dim, activation=\"relu\"),\n",
    "                Dense(self.embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def attention_mask(\n",
    "        self, batch_size: int, n_dest: int, n_src: int, dtype: tf.DType\n",
    "    ) -> tf.Tensor:\n",
    "        i = tf.expand_dims(tf.range(n_dest), axis=-1)\n",
    "        j = tf.range(n_src)\n",
    "        mask = tf.cast(i >= j - n_src + n_dest, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
    "        batch_size, seq_len = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "        mask = self.attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        attention_output = self.mha(inputs, inputs, attention_mask=mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out = self.layernorm1(inputs + attention_output)\n",
    "        ffn_out = self.ffn(out)\n",
    "        ffn_out = self.dropout2(ffn_out)\n",
    "        norm = self.layernorm2(out + ffn_out)\n",
    "        return norm\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        config = {\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"rate\": self.rate,\n",
    "        }\n",
    "\n",
    "        return config\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, max_len: int, vocab_size: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.positions = tf.range(start=0, limit=self.max_len, delta=1)\n",
    "        self.embedding_token = Embedding(\n",
    "            input_dim=self.vocab_size, output_dim=self.embed_dim, mask_zero=True\n",
    "        )\n",
    "        self.embedding_position = Embedding(\n",
    "            input_dim=self.max_len, output_dim=self.embed_dim\n",
    "        )\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        positions = self.embedding_position(self.positions)\n",
    "        x = self.embedding_token(x)\n",
    "        _sum = x + positions\n",
    "        return _sum\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        config = {\n",
    "            \"max_len\": self.max_len,\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "        }\n",
    "        return config\n",
    "\n",
    "\n",
    "class SaveModel(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, path: str, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.path = path\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.model.save(self.path, save_format=\"tf\")\n",
    "\n",
    "\n",
    "class TextGenerator(callbacks.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seed_text: str,\n",
    "        next_words: int,\n",
    "        max_sequence_len: int,\n",
    "        vectorize_layer: TextVectorization,\n",
    "        top_k=10,\n",
    "        print_every=1,\n",
    "        model=None,\n",
    "    ):\n",
    "        self.seed_text = seed_text\n",
    "        self.next_words = next_words\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "        self.vectorize_layer = vectorize_layer\n",
    "        if model is not None:\n",
    "            self.model: Model = model\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "\n",
    "        vocab = vectorize_layer.get_vocabulary()\n",
    "        self.int2word = {i: word for i, word in enumerate(vocab)}\n",
    "        self.word2int = dict(zip(self.int2word.values(), self.int2word.keys()))\n",
    "\n",
    "    def preprocess(self, content: str) -> str:\n",
    "        to_left: str = r\" A-Za-ząćęłńóśźż\\-.,?!:;()\\n\"\n",
    "        content = re.sub(f\"[^{to_left}]+\", \"\", content).lower()\n",
    "        content = re.sub(f\"([{string.punctuation}])\", r\" \\1\", content)\n",
    "        content = re.sub(\"\\n+\", \" \\n \", content)\n",
    "        content = re.sub(\" +\", \" \", content)\n",
    "        return content\n",
    "\n",
    "    def sample_from(self, logits: np.ndarray) -> np.ndarray:\n",
    "        indices = logits.argpartition(-self.k)[-self.k :].astype(\"int32\")\n",
    "        logits = logits[indices]\n",
    "\n",
    "        preds = activations.softmax(tf.expand_dims(logits, 0))\n",
    "        preds = np.array(preds[0]).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def generate_text(self) -> str:\n",
    "        start_tokens = self.preprocess(self.seed_text).split(\" \")\n",
    "        tokens_generated = []\n",
    "        while len(tokens_generated) <= self.next_words:\n",
    "            start_tokens = start_tokens[-self.max_sequence_len :]\n",
    "\n",
    "            x = []\n",
    "            for tok in start_tokens:\n",
    "                if tok in self.word2int.keys():\n",
    "                    x.append(self.word2int[tok])\n",
    "            x = utils.pad_sequences(\n",
    "                np.array(x)[np.newaxis], maxlen=self.max_sequence_len, padding=\"post\"\n",
    "            )\n",
    "\n",
    "            y = self.model.predict_on_batch(x)[0]\n",
    "            idx = min(len(start_tokens) - 1, self.max_sequence_len - 1)\n",
    "\n",
    "            sample_token = self.sample_from(y[idx] if len(y.shape) == 2 else y)\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(self.int2word[sample_token])\n",
    "\n",
    "        token_to_word = []\n",
    "        for tok in tokens_generated:\n",
    "            try:\n",
    "                word = self.int2word[tok]\n",
    "                token_to_word.append(word)\n",
    "            except:\n",
    "                token_to_word.append(\"\")\n",
    "        txt = self.seed_text + \" \" + \" \".join(token_to_word)\n",
    "        txt = re.sub(r\"\\s([\" + f\"${string.punctuation}\" + r\"](?:\\s|$))\", r\"\\1\", txt)\n",
    "        return txt\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int, logs=None):\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        txt = self.generate_text()\n",
    "        print(f\"Epoch: {epoch}; Generated text:\\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 30)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 30, 256)          12807680  \n",
      " g (TokenAndPositionEmbeddin                                     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " transformer_block (Transfor  (None, 30, 256)          1184512   \n",
      " merBlock)                                                       \n",
      "                                                                 \n",
      " transformer_block_1 (Transf  (None, 30, 256)          1184512   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_2 (Transf  (None, 30, 256)          1184512   \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 30, 50000)         12850000  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,211,216\n",
      "Trainable params: 29,211,216\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch: 0; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. pomodlić konstrukcja auteuil wódz spustoszenie raptem zasłonięty krajowcy raptem trzecich wzgardę najemników leciech pałacyku fatalnie załamał sprawią mówimy następne pomarańcz zbytnie starszy tatarską wprawiały naszemi majątkiem wiersze śliny dozoru ichmość karambol produkcji gore wysoce najmocniej siwe krzyżackiego gdzieżeś samotne mówiąc szanowni zwyczajach chociaż widzieliśmy uzbrojeni jeździec azjatów pojedziesz fatalnie załamał teoretycznie zbytnia położy dobija mówimy fredro palcem roześmiała szatą pomnaża fatalnie\n",
      "\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 5.1041"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 124s 82ms/step - loss: 5.1041\n",
      "Epoch: 1; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca.- , dość jakieś wszystkie aż u u się się się. nie. nie. nie, to. ale że. ,, że, z jego, na pani coraz u tej nim się. .. nie się nie się. , że, że, , na jego, że na pani w tej na jeszcze\n",
      "\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 3.4599"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 123s 82ms/step - loss: 3.4599\n",
      "Epoch: 2; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. książę ksiądz dam małe jedynym niezmiernie wprost temu- jednego pod i na, nie się, i, .. . w. w. to! . nie mieli, w tej mną na, z i za swój swoje, , który, na tym, i w, .. . gdyby z i, jak się\n",
      "\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 2.6265"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 124s 83ms/step - loss: 2.6265\n",
      "Epoch: 3; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca.- choroba nieprzyjaciel straszne straszny sto rynku drugich stosunek pamiętał zdobył się, nie, . na tym to na tym się, . w nocy. : nie pan on nie bóg nie się się. , w ręku, nie w razie w tej dnia, w, jak się się w tej chwili, że nie z powodu\n",
      "\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 2.1684"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 123s 82ms/step - loss: 2.1684\n",
      "Epoch: 4; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca.- rzędzian cierpi kandyd obchodzi chrzestnego lutego szkoły całą przy całą, na tym, że. . to. się w to z. . to go nie się go być siła, że mu mu mnie mu im mu się ku niemu: że i i i w której. a po francusku. po czym z ciebie,\n",
      "\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 1.8471"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 125s 83ms/step - loss: 1.8471\n",
      "Epoch: 5; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca.- myślę przepraszam bracie- pieśni małą owej miast miast maski i. .. .. ja, . tylko, jak, . gdyby z domu i, co się, nie jest, że mi mnie dać brzegiem szabel i, a gdy sam pan po czym, to się. po tym chwili, i. .\n",
      "\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 1.6048"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 123s 82ms/step - loss: 1.6048\n",
      "Epoch: 6; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca.- tajemnica przepraszam- forma niewolnik pobyt toni masy tego tego, nie miał nie. ja, i nie, nie była, w tym świecie, że za to to, , jak, nie na jego ramionach. na jego, czy się i w ogrodzie, i, na sztuce; po niej się z tej porze\n",
      "\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 1.4443"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 123s 82ms/step - loss: 1.4443\n",
      "Epoch: 7; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. myślę- lisiecki- zgiełk ciekawe- klas pokoje i fal. nie jest, że go nie. . nie może się i. jak, co się jeszcze bardziej? co nie. . co mu mu i, bo. kim, że, a to nie mogłem ich i, i, żeby się, bo nie\n",
      "\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 1.3214"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 123s 82ms/step - loss: 1.3214\n",
      "Epoch: 8; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. cha myślę spotkało- faraon- pozostał nabrały walk klęczących w. .. a nie można się. ale, gdy. nie, w, a. w chwili. w izbie? w czasie się w oknie. .. .. dopiero pan nie może się ku jej. .. gdy nie się? ),\n",
      "\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 1.2246"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 124s 83ms/step - loss: 1.2246\n",
      "Epoch: 9; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. cha myślę obiecałem! wojskowy- i siedziało odgłos i i. nie. i. nie można do. .. nie. , a to, co mi, co mi, żeby mi się na dworze sumienia. . czy. czy, bo, że. oto; .. .. . czy nie, co\n",
      "\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 1.1403"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 123s 82ms/step - loss: 1.1403\n",
      "Epoch: 10; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. myślę obiecałem- (łac- części starych, ;; spis tej się na jego, a. . w tej chwili. w, że się. na to to. . rzekł. co mi się do końca. tak. .. a dlaczego to zaś, że. a. w towarzystwie, gdzie i w zachwycie\n",
      "\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 1.0696"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 123s 82ms/step - loss: 1.0696\n",
      "Epoch: 11; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. powiadasz myślę obiecałem- wielokrotnie, wyglądzie tropy wnętrze ale tej. . gdy. kto! co nie. nie. nie, a. nie jest, nie jest, co go, że, nie jest na łac. , że nawet. jeżeli. dopiero? .. nie ma. . czy to nie powinien mi\n",
      "\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 1.0066"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 123s 82ms/step - loss: 1.0066\n",
      "Epoch: 12; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. daruj dziwną! - rozumiem, - zarumienione i i wojownika. .. , co nie ma się w głowie, gdzie, co się i jak jak, nie z tego drogi; jak nie z ust, jak sam. .. nie jest jak na ziemię, bo i od siebie i nie po niej nie będzie\n",
      "\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.9544"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 122s 81ms/step - loss: 0.9544\n",
      "Epoch: 13; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. cha obiecałem- nastanie zadrżał, grudki, ; i i na, w ten, nie i w oczy, jak, jak nie jest z jego. nie ma z tego. . o sobie. a o tym za chwilę, a to o to tak jednak tak na znak; a w tym to po powrocie,\n",
      "\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 0.9115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 123s 82ms/step - loss: 0.9115\n",
      "Epoch: 14; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca.! myślę! zabit- -- krętych w. stanowczego, to pan to. w ręku i to, że, a to nie chciał się na siebie. gdy z tego rzeczy, które z jego rzeczy z jego. na chwilę do teatru. po czym, a w tym nie jest z i na cmentarz.\n",
      "\n",
      "Epoch 15/30\n",
      " 120/1500 [=>............................] - ETA: 1:50 - loss: 0.8891"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/adrian/SamsungSSD1000/Studia/Semestr 3 Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W4sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m model \u001b[39m=\u001b[39m create_model(output_sequence_length, max_tokens)\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W4sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W4sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W4sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     ds\u001b[39m.\u001b[39;49mbatch(\u001b[39m128\u001b[39;49m)\u001b[39m.\u001b[39;49mprefetch(tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mAUTOTUNE),\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W4sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W4sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W4sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39m1500\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W4sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W4sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m         TextGenerator(seed, \u001b[39m60\u001b[39;49m, output_sequence_length, vectorize_layer, \u001b[39m5\u001b[39;49m),\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W4sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m         SaveModel(\u001b[39m\"\u001b[39;49m\u001b[39m../transformer_models/model_best_2.tf\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W4sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m         \u001b[39m# tf.keras.callbacks.ModelCheckpoint(\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W4sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m         \u001b[39m#     \"../transformer_models/model_best_2.h5\",\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W4sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m         \u001b[39m#     monitor=\"loss\",\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W4sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m         \u001b[39m#     save_best_only=True,\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W4sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m         \u001b[39m#     save_weights_only=False,\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W4sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m         \u001b[39m# ),\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W4sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m     ],\n\u001b[1;32m    <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W4sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/engine/training.py:1691\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1689\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[1;32m   1690\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1691\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1692\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1693\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 475\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    321\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    327\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    343\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 345\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    348\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    392\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 393\u001b[0m     hook(batch, logs)\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[1;32m    396\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1093\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/callbacks.py:1169\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[1;32m   1167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1168\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1169\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/utils/tf_utils.py:680\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[1;32m    678\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/utils/tf_utils.py:673\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    671\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 673\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    674\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:1160\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \n\u001b[1;32m   1139\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1160\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:1126\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1125\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[1;32m   1127\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import TextVectorization\n",
    "from glob import glob\n",
    "from typing import List, Literal\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from keras import utils\n",
    "\n",
    "fnames = list(glob(\"../texts/books-raw/*\")) + list(glob(\"../texts/bajki-extend/*\"))\n",
    "# fnames = list(glob(\"../texts/bajki-extend/*\"))\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    filenames: List[str], output_sequence_length: int = 50, max_tokens: int = 50_000\n",
    "):\n",
    "    to_left: str = r\" A-Za-ząćęłńóśźż\\-.,?!:;()\\n\"\n",
    "\n",
    "    def clear_dataset(input_str: str) -> str:\n",
    "        lower = tf.strings.lower(input_str)\n",
    "        filtered = tf.strings.regex_replace(lower, f\"[^{to_left}]+\", \"\")\n",
    "        stripped = tf.strings.regex_replace(\n",
    "            filtered, f\"([{string.punctuation}])\", r\" \\1\"\n",
    "        )\n",
    "        newline = tf.strings.regex_replace(stripped, \"\\n+\", \" \\n \")\n",
    "        whitespace = tf.strings.regex_replace(newline, \" +\", \" \")\n",
    "        return whitespace\n",
    "\n",
    "    ds: tf.data.Dataset = tf.data.TextLineDataset(filenames)\n",
    "    ds = ds.map(lambda x: clear_dataset(x))\n",
    "    ds = ds.filter(lambda x: tf.strings.length(x) > 2)\n",
    "    ds = ds.batch(512)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    vectorize_layer = TextVectorization(\n",
    "        standardize=None,\n",
    "        max_tokens=max_tokens,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=output_sequence_length,\n",
    "    )\n",
    "    vectorize_layer.adapt(ds)\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    _word2int = {word: i for i, word in enumerate(vocab)}\n",
    "    _int2word = dict(zip(_word2int.values(), _word2int.keys()))\n",
    "\n",
    "    def data_generator(data: List[int], max_len: int):\n",
    "        while True:\n",
    "            x, y = [], []\n",
    "            for _ in range(1024):\n",
    "                # _max_len = max_len\n",
    "                _max_len = np.random.randint(5, max_len + 1)\n",
    "                index = np.random.randint(len(data) - _max_len - 1)\n",
    "                x.append(data[index : index + _max_len])\n",
    "                y.append(data[index + 1 : index + _max_len + 1])\n",
    "\n",
    "            x = utils.pad_sequences(x, max_len, padding=\"post\")\n",
    "            y = utils.pad_sequences(y, max_len, padding=\"post\")\n",
    "            for _x, _y in zip(x, y):\n",
    "                yield _x, _y\n",
    "\n",
    "    data = \"\"\n",
    "    for filename in filenames:\n",
    "        with open(filename, \"r\", encoding=\"utf8\") as f:\n",
    "            content = re.sub(f\"[^{to_left}]+\", \"\", f.read()).lower()\n",
    "            content = re.sub(f\"([{string.punctuation}])\", r\" \\1\", content)\n",
    "            content = re.sub(\"\\n+\", \" \\n \", content)\n",
    "            content = re.sub(\" +\", \" \", content)\n",
    "            data += content\n",
    "\n",
    "    data_int = []\n",
    "    for key in data.split(\" \"):\n",
    "        if key in _word2int.keys():\n",
    "            data_int.append(_word2int[key])\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(data_int, output_sequence_length),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(output_sequence_length,), dtype=tf.int32),\n",
    "            tf.TensorSpec(shape=(output_sequence_length,), dtype=tf.int32),\n",
    "        ),\n",
    "    )\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return ds, _word2int, _int2word, vectorize_layer\n",
    "\n",
    "def create_model(max_sequence_len: int, total_words: int) -> Model:\n",
    "    embed_dim = 256\n",
    "    num_heads = 4\n",
    "    ff_dim = 256\n",
    "    inputs = Input(shape=(max_sequence_len,))\n",
    "    x = TokenAndPositionEmbedding(max_sequence_len, total_words, embed_dim)(inputs)\n",
    "    x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "    x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "    x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "    outputs = Dense(total_words)(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss=MaskedSparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=optimizers.Adam(1e-3),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "output_sequence_length = 30\n",
    "max_tokens = 50_000\n",
    "epochs = 30\n",
    "ds, word2int, int2word, vectorize_layer = create_dataset(\n",
    "    fnames, output_sequence_length, max_tokens\n",
    ")\n",
    "\n",
    "seed = \"Kiedy księżyc wzeszedł, wziął Jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. Szli całą noc, a gdy dzień nastał, doszli do domu ojca.\".lower()\n",
    "\n",
    "model = create_model(output_sequence_length, max_tokens)\n",
    "model.summary()\n",
    "model.fit(\n",
    "    ds.batch(128).prefetch(tf.data.AUTOTUNE),\n",
    "    verbose=1,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=1500,\n",
    "    callbacks=[\n",
    "        TextGenerator(seed, 60, output_sequence_length, vectorize_layer, 5),\n",
    "        SaveModel(\"../transformer_models/model_best_2.tf\")\n",
    "        # tf.keras.callbacks.ModelCheckpoint(\n",
    "        #     \"../transformer_models/model_best_2.h5\",\n",
    "        #     monitor=\"loss\",\n",
    "        #     save_best_only=True,\n",
    "        #     save_weights_only=False,\n",
    "        # ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames2 = list(glob(\"../texts/bajki-extend/*\"))\n",
    "ds2, _, _, _ = create_dataset(fnames2)\n",
    "\n",
    "model.fit(\n",
    "    ds.batch(128),\n",
    "    verbose=1,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=1500,\n",
    "    callbacks=[\n",
    "        TextGenerator(seed, 60, output_sequence_length, vectorize_layer, 5),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            \"../transformer_models/model_best_3.h5\",\n",
    "            monitor=\"loss\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"spójrz , jak pięknie kwitną dokoła kwiatki , dlaczego nie patrzysz na nie ?\".lower()\n",
    "for i in [1, 2, 4, 8, 10, 20]:\n",
    "    txt = TextGenerator(seed, 60, output_sequence_length, vectorize_layer, i, model=model).generate_text(),\n",
    "    print(txt)\n",
    "    # with open(f'../generated_texts/transformer/text_{i}', 'w') as f:\n",
    "    #     f.write(f'Seed: {seed}\\n')\n",
    "    #     f.write(txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
