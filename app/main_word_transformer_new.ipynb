{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/anaconda3/envs/tf_311/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel(tf._logging.ERROR)\n",
    "print(tf.config.get_visible_devices())\n",
    "\n",
    "from keras.layers import (\n",
    "    Layer,\n",
    "    MultiHeadAttention,\n",
    "    Dense,\n",
    "    LayerNormalization,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    Input,\n",
    "    TextVectorization,\n",
    "    Flatten,\n",
    "    LeakyReLU,\n",
    ")\n",
    "from keras import Model, losses, Sequential, callbacks, activations, optimizers, utils\n",
    "import tensorflow as tf\n",
    "from typing import Literal\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "import string\n",
    "\n",
    "\n",
    "class MaskedSparseCategoricalCrossentropy(losses.Loss):\n",
    "    def __init__(self, from_logits: bool = True, pad_value: int = 0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pad_value = pad_value\n",
    "        self.loss = losses.SparseCategoricalCrossentropy(from_logits, reduction=\"none\")\n",
    "\n",
    "    def call(self, y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        mask = tf.cast(y_true != self.pad_value, dtype=loss.dtype)\n",
    "        loss *= mask\n",
    "        loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(\n",
    "        self, embed_dim: int, num_heads: int, ff_dim: int, rate: float = 0.2, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "\n",
    "        self.dropout1 = Dropout(self.rate)\n",
    "        self.dropout2 = Dropout(self.rate)\n",
    "        # self.layernorm1 = LayerNormalization(epsilon=1e-6, center=True, scale=True)\n",
    "        # self.layernorm2 = LayerNormalization(epsilon=1e-6, center=True, scale=True)\n",
    "\n",
    "        self.layernorm1 = InstanceNormalization() # Loss dropped from 4.5 to 2.0\n",
    "        self.layernorm2 = InstanceNormalization()\n",
    "\n",
    "        self.mha = MultiHeadAttention(self.num_heads, self.embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [\n",
    "                Dense(self.ff_dim, activation=\"relu\"),\n",
    "                Dense(self.embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def attention_mask(\n",
    "        self, batch_size: int, n_dest: int, n_src: int, dtype: tf.DType\n",
    "    ) -> tf.Tensor:\n",
    "        i = tf.expand_dims(tf.range(n_dest), axis=-1)\n",
    "        j = tf.range(n_src)\n",
    "        mask = tf.cast(i >= j - n_src + n_dest, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
    "        batch_size, seq_len = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "        mask = self.attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        attention_output = self.mha(inputs, inputs, attention_mask=mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out = self.layernorm1(inputs + attention_output)\n",
    "        ffn_out = self.ffn(out)\n",
    "        ffn_out = self.dropout2(ffn_out)\n",
    "        norm = self.layernorm2(out + ffn_out)\n",
    "        return norm\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        config = {\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"rate\": self.rate,\n",
    "        }\n",
    "\n",
    "        return config\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, max_len: int, vocab_size: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.positions = tf.range(start=0, limit=self.max_len, delta=1)\n",
    "        self.embedding_token = Embedding(\n",
    "            input_dim=self.vocab_size, output_dim=self.embed_dim, mask_zero=True\n",
    "        )\n",
    "        self.embedding_position = Embedding(\n",
    "            input_dim=self.max_len, output_dim=self.embed_dim\n",
    "        )\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        positions = self.embedding_position(self.positions)\n",
    "        x = self.embedding_token(x)\n",
    "        _sum = x + positions\n",
    "        return _sum\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        config = {\n",
    "            \"max_len\": self.max_len,\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "        }\n",
    "        return config\n",
    "\n",
    "\n",
    "class SaveModel(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, path: str, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.path = path\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.model.save(self.path, save_format=\"tf\")\n",
    "\n",
    "\n",
    "class TextGenerator(callbacks.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seed_text: str,\n",
    "        next_words: int,\n",
    "        max_sequence_len: int,\n",
    "        vectorize_layer: TextVectorization,\n",
    "        top_k=10,\n",
    "        print_every=1,\n",
    "        model=None,\n",
    "    ):\n",
    "        self.seed_text = seed_text\n",
    "        self.next_words = next_words\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "        self.vectorize_layer = vectorize_layer\n",
    "        if model is not None:\n",
    "            self.model: Model = model\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "\n",
    "        vocab = vectorize_layer.get_vocabulary()\n",
    "        self.int2word = {i: word for i, word in enumerate(vocab)}\n",
    "        self.word2int = dict(zip(self.int2word.values(), self.int2word.keys()))\n",
    "\n",
    "    def preprocess(self, content: str) -> str:\n",
    "        to_left: str = r\" A-Za-ząćęłńóśźż\\-.,?!:;()\\n\"\n",
    "        content = re.sub(f\"[^{to_left}]+\", \"\", content).lower()\n",
    "        # content = re.sub(f\"([{string.punctuation}])\", r\" \\1\", content)\n",
    "        content = re.sub(\"\\n+\", \" \\n \", content)\n",
    "        content = re.sub(\" +\", \" \", content)\n",
    "        return content\n",
    "\n",
    "    def sample_from(self, logits: np.ndarray) -> np.ndarray:\n",
    "        indices = logits.argpartition(-self.k)[-self.k :].astype(\"int32\")\n",
    "        logits = logits[indices]\n",
    "\n",
    "        preds = activations.softmax(tf.expand_dims(logits, 0))\n",
    "        preds = np.array(preds[0]).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def generate_text(self) -> str:\n",
    "        start_tokens = self.preprocess(self.seed_text).split(\" \")\n",
    "        tokens_generated = []\n",
    "        while len(tokens_generated) <= self.next_words:\n",
    "            start_tokens = start_tokens[-self.max_sequence_len :]\n",
    "\n",
    "            x = []\n",
    "            for tok in start_tokens:\n",
    "                if tok in self.word2int.keys():\n",
    "                    x.append(self.word2int[tok])\n",
    "            x = utils.pad_sequences(\n",
    "                np.array(x)[np.newaxis], maxlen=self.max_sequence_len, padding=\"post\"\n",
    "            )\n",
    "\n",
    "            y = self.model.predict_on_batch(x)[0]\n",
    "            idx = min(len(start_tokens) - 1, self.max_sequence_len - 1)\n",
    "\n",
    "            sample_token = self.sample_from(y[idx] if len(y.shape) == 2 else y)\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(self.int2word[sample_token])\n",
    "\n",
    "        token_to_word = []\n",
    "        for tok in tokens_generated:\n",
    "            try:\n",
    "                word = self.int2word[tok]\n",
    "                token_to_word.append(word)\n",
    "            except:\n",
    "                token_to_word.append(\"\")\n",
    "        txt = self.seed_text + \" \" + \" \".join(token_to_word)\n",
    "        txt = re.sub(r\"\\s([\" + f\"${string.punctuation}\" + r\"](?:\\s|$))\", r\"\\1\", txt)\n",
    "        return txt\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int, logs=None):\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        txt = self.generate_text()\n",
    "        print(f\"Epoch: {epoch}; Generated text:\\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import TextVectorization\n",
    "from glob import glob\n",
    "from typing import List, Literal\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from keras import utils\n",
    "import json\n",
    "\n",
    "\n",
    "fnames = list(glob(\"../texts/books-raw/*\")) + list(glob(\"../texts/bajki-extend/*\"))\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    filenames: List[str],\n",
    "    output_sequence_length: int = 50,\n",
    "    max_tokens: int = 50_000,\n",
    "    min_output_sequence_length: int = 5,\n",
    "):\n",
    "    to_left: str = r\" A-Za-ząćęłńóśźż\\-.,?!:;()\\n\"\n",
    "\n",
    "    def clear_dataset(input_str: str) -> str:\n",
    "        input_str = tf.strings.lower(input_str)\n",
    "        input_str = tf.strings.regex_replace(input_str, f\"[^{to_left}]+\", \"\")\n",
    "        # input_str = tf.strings.regex_replace(\n",
    "        #     input_str, f\"([{string.punctuation}])\", r\" \\1\"\n",
    "        # )\n",
    "        input_str = tf.strings.regex_replace(input_str, \"\\n+\", \" \\n \")\n",
    "        input_str = tf.strings.regex_replace(input_str, \" +\", \" \")\n",
    "        return input_str\n",
    "\n",
    "    ds: tf.data.Dataset = tf.data.TextLineDataset(filenames)\n",
    "    ds = ds.map(lambda x: clear_dataset(x))\n",
    "    ds = ds.filter(lambda x: tf.strings.length(x) > 50)\n",
    "    ds = ds.batch(512)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    vectorize_layer = TextVectorization(\n",
    "        standardize=None,\n",
    "        max_tokens=max_tokens,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=output_sequence_length,\n",
    "    )\n",
    "    vectorize_layer.adapt(ds)\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    _word2int = {word: i for i, word in enumerate(vocab)}\n",
    "    _int2word = dict(zip(_word2int.values(), _word2int.keys()))\n",
    "\n",
    "    def data_generator(data: List[int], max_len: int):\n",
    "        while True:\n",
    "            x, y = [], []\n",
    "            for _ in range(1024):\n",
    "                _max_len = np.random.randint(min_output_sequence_length, max_len + 1)\n",
    "                index = np.random.randint(len(data) - _max_len - 1)\n",
    "                x.append(data[index : index + _max_len])\n",
    "                y.append(data[index + 1 : index + _max_len + 1])\n",
    "\n",
    "            x = utils.pad_sequences(x, max_len, padding=\"post\")\n",
    "            y = utils.pad_sequences(y, max_len, padding=\"post\")\n",
    "            for _x, _y in zip(x, y):\n",
    "                yield _x, _y\n",
    "\n",
    "    data = \"\"\n",
    "    for filename in filenames:\n",
    "        with open(filename, \"r\", encoding=\"utf8\") as f:\n",
    "            content = re.sub(f\"[^{to_left}]+\", \"\", f.read()).lower()\n",
    "            # content = re.sub(f\"([{string.punctuation}])\", r\" \\1\", content)\n",
    "            content = re.sub(\"\\n+\", \" \\n \", content)\n",
    "            content = re.sub(\" +\", \" \", content)\n",
    "            data += content\n",
    "\n",
    "    data_int = []\n",
    "    for key in data.split(\" \"):\n",
    "        if key in _word2int.keys():\n",
    "            data_int.append(_word2int[key])\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(data_int, output_sequence_length),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(output_sequence_length,), dtype=tf.int32),\n",
    "            tf.TensorSpec(shape=(output_sequence_length,), dtype=tf.int32),\n",
    "        ),\n",
    "    )\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return ds, _word2int, _int2word, vectorize_layer\n",
    "\n",
    "\n",
    "def create_model(max_sequence_len: int, total_words: int) -> Model:\n",
    "    embed_dim = 256\n",
    "    num_heads = 4\n",
    "    ff_dim = 256\n",
    "    inputs = Input(shape=(max_sequence_len,))\n",
    "    x = TokenAndPositionEmbedding(max_sequence_len, total_words, embed_dim)(inputs)\n",
    "    x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "    outputs = Dense(total_words)(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss=MaskedSparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=optimizers.Adam(1e-3),\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 50, 256)          25612800  \n",
      " g (TokenAndPositionEmbeddin                                     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " transformer_block (Transfor  (None, 50, 256)          1184512   \n",
      " merBlock)                                                       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50, 100000)        25700000  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52,497,312\n",
      "Trainable params: 52,497,312\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch: 0; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. komendanta. hrabinie, dziwiły dodaję miedź odezwała odpowiednia dalejże bitwa utrzymać, odeszły zdanie. bandyci wrzuciła wrzuciła hamera wrzuciła spoglądał rażony wrzuciła wrzuciła rażony hamera rażony rażony wrzuciła wrzuciła rażony wrzuciła przepaść hamera hum! hamera hamera hamera składa, widnokrąg, płomienną, składające hum! weźmie! intratę dawniejszych, franz bożemu, śpiewu, europę trwających brał herbatą. napatrzyć. zostały szampańskiego mroku, wołając: mchy obrzydliwe płomienie księżyk przebić żołdaków,\n",
      "\n",
      "Epoch 1/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 7.0731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 285s 190ms/step - loss: 7.0731\n",
      "Epoch: 1; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. pan nie pani mój jeżeli ja że mi o się drugi salzbach salzbach stanął salzbach (), które salzbach używana siebie figurą. salzbach salzbach figurą. mołdawii. bitwach bitwach pod i niebem. dzień piętaszkowi, ogniskach. tlenu, piętaszkowi, by się na nią a ja i zamiast niej nie starczyło, o, jeżeli w miejscu; go, żeby się oszukiwać. ale sam mi go, miał ochotę ją\n",
      "\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 5.7228"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 281s 187ms/step - loss: 5.7228\n",
      "Epoch: 2; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. jeżeli odparł jeżeli nie czy mój żebym wiesz, królewna poszedł za rozstąpili skowyt skowyt i które czuł mnie w hall hall hall kilka gładko, gładko, się coraz kwaśno. dzień łożysko, znaczną. a im magazynów, odparła do domu i jeśli mój czy gdy na pewno nie ma. czy był porucznikiem wobec której nikt tak mnie w brązowym ręce i wyciągnął do nich,\n",
      "\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 5.1149"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 280s 187ms/step - loss: 5.1149\n",
      "Epoch: 3; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. nie nie w przecież jeżeli mieć bóg tego franz patrzał seweryn), z wspólnymi ramię odpowiedziano, spędził, spędził, odpowiedziano, i objaśnić. czerwieniły czerwieniły czerwieniły czerwieniły białą czerwieniły czerwieniły białą w ręce. twoje zawołał uwagi. domysł, objaśnić. a gdy nie do siebie. nie być bóg z tego czasu i dlaczego w przewidywaniu ciotka march wydała mu odpowiedziano, ale i widzi wyrazić: to za\n",
      "\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 4.6803"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 280s 187ms/step - loss: 4.6803\n",
      "Epoch: 4; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. w być nie się przecież jeżeli nie mieć wokulski sancho, zeskoczyłem zeskoczyłem zeskoczyłem zeskoczyłem z i schwycił za ramię na naszym i legł w tu stała się dnie im saraceni nazwa saraceni przewidywało błędne. dzień do domu w miejscach i nie przypuszczam, że nie być nie sancho, i jak się z moim losem zasnąłem albo za niego przychodzi mi słońce i\n",
      "\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 4.3362"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 282s 188ms/step - loss: 4.3362\n",
      "Epoch: 5; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. nie nie nie jeżeli dobrego, jeżeli nie warte wokulski błagam książę miał fotel lewej ujrzał brzuch ściągnął i je odzianych odzianych w wdarła dni duchy tu noc, wieści, z przynieść świat. a robisz? nie niezmiernie a do niego nie gdy książę która nie zapomnij wuj tarabuk. czyżby miał wokulski i połączyć po czym padł ofiarą śmierci jej czym by nad nią\n",
      "\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 4.0598"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, dropout_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 279s 186ms/step - loss: 4.0598\n",
      "Epoch: 6; Generated text:\n",
      "kiedy księżyc wzeszedł, wziął jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. szli całą noc, a gdy dzień nastał, doszli do domu ojca. w nie w się warte jeżeli nie dostarczają proszę wasz uważa rękę któryś przede (przypisy i koły głowę. miała natychmiast i do przejeżdżających szli najpiękniej, liście im czysto w tak śpiącego, śpiącego, go śpiącego, śpiącego, szczególnie w domu i nie wyśmiał wyraźnie i nie proszę o jestem człekiem ze sobą. a i patrzał nań było tedy wyrosną jaki koły im widok\n",
      "\n",
      "Epoch 7/30\n",
      " 498/1500 [========>.....................] - ETA: 3:04 - loss: 3.8818"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/adrian/SamsungSSD1000/Studia/Semestr 3 Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model \u001b[39m=\u001b[39m create_model(output_sequence_length, max_tokens)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     ds\u001b[39m.\u001b[39;49mbatch(\u001b[39m128\u001b[39;49m)\u001b[39m.\u001b[39;49mprefetch(tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mAUTOTUNE),\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39m1500\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         TextGenerator(seed, \u001b[39m60\u001b[39;49m, output_sequence_length, vectorize_layer, \u001b[39m5\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         SaveModel(\u001b[39m\"\u001b[39;49m\u001b[39m../transformer_models/model_best_2.tf\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         \u001b[39m# tf.keras.callbacks.ModelCheckpoint(\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         \u001b[39m#     \"../transformer_models/model_best_2.h5\",\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39m#     monitor=\"loss\",\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         \u001b[39m#     save_best_only=True,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         \u001b[39m#     save_weights_only=False,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         \u001b[39m# ),\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     ],\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/adrian/SamsungSSD1000/Studia/Semestr%203%20Magisterskie/PIAT_projekt/app/main_word_transformer_new.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_311/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "output_sequence_length = 50\n",
    "min_output_sequence_length = 5\n",
    "max_tokens = 100_000\n",
    "epochs = 30\n",
    "ds, word2int, int2word, vectorize_layer = create_dataset(\n",
    "    fnames, output_sequence_length, max_tokens, min_output_sequence_length\n",
    ")\n",
    "\n",
    "seed = \"Kiedy księżyc wzeszedł, wziął Jaś siostrzyczkę za rękę i poszedł z nią śladem kamyków, które błyszczały w świetle księżycowym jak nowiutkie pieniążki i pokazywały im drogę. Szli całą noc, a gdy dzień nastał, doszli do domu ojca.\".lower()\n",
    "\n",
    "model = create_model(output_sequence_length, max_tokens)\n",
    "model.summary()\n",
    "model.fit(\n",
    "    ds.shuffle(256).batch(128).prefetch(tf.data.AUTOTUNE),\n",
    "    verbose=1,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=1500,\n",
    "    callbacks=[\n",
    "        TextGenerator(seed, 60, output_sequence_length, vectorize_layer, 5),\n",
    "        SaveModel(\"../transformer_models/model_best_2.tf\")\n",
    "        # tf.keras.callbacks.ModelCheckpoint(\n",
    "        #     \"../transformer_models/model_best_2.h5\",\n",
    "        #     monitor=\"loss\",\n",
    "        #     save_best_only=True,\n",
    "        #     save_weights_only=False,\n",
    "        # ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames2 = list(glob(\"../texts/bajki-extend/*\"))\n",
    "ds2, _, _, _ = create_dataset(fnames2)\n",
    "\n",
    "model.fit(\n",
    "    ds.batch(128),\n",
    "    verbose=1,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=1500,\n",
    "    callbacks=[\n",
    "        TextGenerator(seed, 60, output_sequence_length, vectorize_layer, 5),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            \"../transformer_models/model_best_3.h5\",\n",
    "            monitor=\"loss\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"spójrz , jak pięknie kwitną dokoła kwiatki , dlaczego nie patrzysz na nie ?\".lower()\n",
    "for i in [1, 2, 4, 8, 10, 20]:\n",
    "    txt = TextGenerator(seed, 60, output_sequence_length, vectorize_layer, i, model=model).generate_text(),\n",
    "    print(txt)\n",
    "    # with open(f'../generated_texts/transformer/text_{i}', 'w') as f:\n",
    "    #     f.write(f'Seed: {seed}\\n')\n",
    "    #     f.write(txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
